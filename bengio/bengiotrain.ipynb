{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://abhinavcreed13.github.io/blog/bengio-trigram-nplm-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77363\n",
      "9418\n"
     ]
    }
   ],
   "source": [
    "#load wikitext data\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "train_file_path = '../newtraincorpus.txt'\n",
    "test_file_path = '../newtestcorpus.txt'\n",
    "\n",
    "with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "            # Process each line as a sentence\n",
    "            words = (line.strip().split())\n",
    "            train.append(words)\n",
    "\n",
    "with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "            # Process each line as a sentence\n",
    "            words = (line.strip().split())\n",
    "            test.append(words)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['valkyria',\n",
       " 'of',\n",
       " 'the',\n",
       " 'battlefield',\n",
       " '<num>',\n",
       " 'commonly',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'valkyria',\n",
       " 'chronicles',\n",
       " 'iii',\n",
       " 'outside',\n",
       " 'japan',\n",
       " 'is',\n",
       " 'a',\n",
       " 'tactical',\n",
       " 'role',\n",
       " '@-@',\n",
       " 'playing',\n",
       " 'video',\n",
       " 'game',\n",
       " 'developed',\n",
       " 'by',\n",
       " 'sega',\n",
       " 'and',\n",
       " 'media']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19114\n"
     ]
    }
   ],
   "source": [
    "#create vocabulary\n",
    "\n",
    "\n",
    "UNK_symbol = \"<UNK>\"\n",
    "vocab = set([UNK_symbol])\n",
    "\n",
    "\n",
    "\n",
    "# create term frequency of the words\n",
    "words_term_frequency_train = {}\n",
    "for doc in train:\n",
    "    for word in doc:\n",
    "        # this will calculate term frequency\n",
    "        # since we are taking all words now\n",
    "        words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "\n",
    "# create vocabulary\n",
    "for doc in train:\n",
    "    for word in doc:\n",
    "        if words_term_frequency_train.get(word,0) >= 5:\n",
    "            vocab.add(word)\n",
    "\n",
    "# remove \"@-@\" from vocab\n",
    "vocab.remove(\"@-@\")\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1480510, 2)\n",
      "(1480510, 1)\n",
      "(174126, 2)\n",
      "(174126, 1)\n"
     ]
    }
   ],
   "source": [
    "#create trigrams\n",
    "\n",
    "import numpy as np\n",
    "# create required lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "# create word to id mappings\n",
    "word_to_id_mappings = {}\n",
    "for idx,word in enumerate(vocab):\n",
    "    word_to_id_mappings[word] = idx\n",
    "\n",
    "# function to get id for a given word\n",
    "# return <UNK> id if not found\n",
    "def get_id_of_word(word):\n",
    "    unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "    return word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "# creating training and dev set\n",
    "for sentence in train:\n",
    "    for i,word in enumerate(sentence):\n",
    "        if i+2 >= len(sentence):\n",
    "            # sentence boundary reached\n",
    "            # ignoring sentence less than 3 words\n",
    "            break\n",
    "        # convert word to id\n",
    "        x_extract = [get_id_of_word(word),get_id_of_word(sentence[i+1])]\n",
    "        y_extract = [get_id_of_word(sentence[i+2])]\n",
    "\n",
    "        x_train.append(x_extract)\n",
    "        y_train.append(y_extract)\n",
    "\n",
    "for sentence in test:\n",
    "    for i,word in enumerate(sentence):\n",
    "        if i+2 >= len(sentence):\n",
    "            # sentence boundary reached\n",
    "            # ignoring sentence less than 3 words\n",
    "            break\n",
    "        # convert word to id\n",
    "        x_extract = [get_id_of_word(word),get_id_of_word(sentence[i+1])]\n",
    "        y_extract = [get_id_of_word(sentence[i+2])]\n",
    "\n",
    "        x_test.append(x_extract)\n",
    "        y_test.append(y_extract)\n",
    "  \n",
    "\n",
    "# making numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)  \n",
    "  \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import torch\n",
    "import multiprocessing\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram Neural Network Model\n",
    "class TrigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(TrigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "--- Creating training and dev dataloaders with 256 batch size ---\n"
     ]
    }
   ],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 256\n",
    "# hidden units\n",
    "H = 100\n",
    "torch.manual_seed(13013)\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "available_workers = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"--- Creating training and dev dataloaders with {} batch size ---\".format(BATCH_SIZE))\n",
    "train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "test_set = np.concatenate((x_test, y_test), axis=1)\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = available_workers)\n",
    "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, num_workers = available_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get accuracy from log probabilities\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:2].type(torch.LongTensor)\n",
    "            target_tensor = data_tensor[:,2].type(torch.LongTensor)\n",
    "            context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5552, 13071],\n",
      "        [13071, 17778],\n",
      "        [17778, 18091],\n",
      "        [18091, 12167],\n",
      "        [12167,  3047],\n",
      "        [ 3047,  2101],\n",
      "        [ 2101,  5533],\n",
      "        [ 5533,  2148],\n",
      "        [ 2148,  5552],\n",
      "        [ 5552,   590],\n",
      "        [  590, 18441],\n",
      "        [18441,  1888],\n",
      "        [ 1888, 13535],\n",
      "        [13535,   102],\n",
      "        [  102,  2322],\n",
      "        [ 2322, 12841],\n",
      "        [12841, 18311],\n",
      "        [18311,  5328],\n",
      "        [ 5328,  5491],\n",
      "        [ 5491,  9542],\n",
      "        [ 9542,  2586],\n",
      "        [ 2586,  1855],\n",
      "        [ 1855,  8556],\n",
      "        [ 8556, 12548],\n",
      "        [12548,  3402],\n",
      "        [17672,  7354],\n",
      "        [ 7354, 17778],\n",
      "        [17778, 16050],\n",
      "        [ 1617, 15790],\n",
      "        [15790, 12055],\n",
      "        [12055,  5342],\n",
      "        [ 5342, 15790],\n",
      "        [15790, 13535],\n",
      "        [13535,  2940],\n",
      "        [ 2940,   102],\n",
      "        [  102, 17778],\n",
      "        [17778,  9356],\n",
      "        [ 9356,  2586],\n",
      "        [ 2586, 15790],\n",
      "        [15790, 17778],\n",
      "        [17778,  5552],\n",
      "        [ 5267, 17778],\n",
      "        [17778,   204],\n",
      "        [  204,  9913],\n",
      "        [ 9913, 13071],\n",
      "        [13071, 12841],\n",
      "        [12841,  3402],\n",
      "        [ 3402, 15141],\n",
      "        [15141,  5328],\n",
      "        [ 5328,  9615],\n",
      "        [ 9615, 19050],\n",
      "        [19050,  2148],\n",
      "        [ 2148,  3252],\n",
      "        [ 3252,  9850],\n",
      "        [ 9850, 17778],\n",
      "        [17778, 17341],\n",
      "        [17341,  3239],\n",
      "        [ 3239, 18630],\n",
      "        [18630,  5533],\n",
      "        [ 5533, 17778],\n",
      "        [17778,  9429],\n",
      "        [ 9429,  2586],\n",
      "        [ 2586,  3402],\n",
      "        [ 3402,   928],\n",
      "        [  928, 17778],\n",
      "        [17778,  9865],\n",
      "        [ 9865,  2322],\n",
      "        [ 2322, 14458],\n",
      "        [14458, 11257],\n",
      "        [11257, 17134],\n",
      "        [17134, 11572],\n",
      "        [11572, 17778],\n",
      "        [17778,  1128],\n",
      "        [ 1128, 13071],\n",
      "        [13071,  5447],\n",
      "        [ 5447, 15754],\n",
      "        [15754, 17778],\n",
      "        [17778,  4329],\n",
      "        [ 4329,  5328],\n",
      "        [ 5328,  4029],\n",
      "        [ 4029, 18552],\n",
      "        [18552, 10077],\n",
      "        [10077,  1567],\n",
      "        [ 1567, 17769],\n",
      "        [17769,  7151],\n",
      "        [ 7151,  3402],\n",
      "        [ 3402, 15459],\n",
      "        [15459,  1844],\n",
      "        [ 1844, 12730],\n",
      "        [12730, 17778],\n",
      "        [17778,   576],\n",
      "        [  576, 17134],\n",
      "        [17134,  5328],\n",
      "        [17778,  2586],\n",
      "        [ 2586,  1199],\n",
      "        [ 1199, 11833],\n",
      "        [11833, 15790],\n",
      "        [15790,  5342],\n",
      "        [ 5342, 18690],\n",
      "        [18690,  8689],\n",
      "        [ 8689,  2322],\n",
      "        [ 2322, 17352],\n",
      "        [17352,  2060],\n",
      "        [ 2060, 13071],\n",
      "        [13071, 17778],\n",
      "        [17778,  9655],\n",
      "        [ 9655, 11574],\n",
      "        [11574,   913],\n",
      "        [  913,  5552],\n",
      "        [ 5552,   590],\n",
      "        [18706,  2940],\n",
      "        [ 2940,  4634],\n",
      "        [ 4634, 17778],\n",
      "        [17778, 15626],\n",
      "        [15626,  2931],\n",
      "        [ 2931, 13071],\n",
      "        [13071, 17778],\n",
      "        [17778,  1585],\n",
      "        [ 1585,  2940],\n",
      "        [ 2940, 10610],\n",
      "        [10610, 11419],\n",
      "        [11419,  8614],\n",
      "        [ 8614,  6179],\n",
      "        [ 6179, 17444],\n",
      "        [17444,  2148],\n",
      "        [ 2148,   267],\n",
      "        [  267, 17778],\n",
      "        [17778,  2586],\n",
      "        [ 2586, 18592],\n",
      "        [18592,  5328],\n",
      "        [ 5328,  7354],\n",
      "        [ 7354,  1585],\n",
      "        [ 4305,  6505],\n",
      "        [ 6505,  5328],\n",
      "        [ 5328,  5328],\n",
      "        [ 5328,  3402],\n",
      "        [ 3402, 14689],\n",
      "        [14689,  5328],\n",
      "        [ 5328,  5328],\n",
      "        [ 5328,  3735],\n",
      "        [ 3735,  8879],\n",
      "        [ 8879, 10904],\n",
      "        [10904, 11032],\n",
      "        [11032, 12034],\n",
      "        [12034,   272],\n",
      "        [  272,  8838],\n",
      "        [ 8838,  5552],\n",
      "        [ 5552,   590],\n",
      "        [  590,  4653],\n",
      "        [ 4653, 13514],\n",
      "        [13514,  1894],\n",
      "        [ 2322, 17352],\n",
      "        [17352, 14796],\n",
      "        [14796, 13071],\n",
      "        [13071,  7340],\n",
      "        [ 7340,    13],\n",
      "        [   13, 17778],\n",
      "        [17778,  2586],\n",
      "        [ 2586,  3926],\n",
      "        [ 3926,  4788],\n",
      "        [ 4788, 17560],\n",
      "        [17560, 11597],\n",
      "        [11597, 12049],\n",
      "        [12049,  8556],\n",
      "        [ 8556, 14403],\n",
      "        [ 2940,  5303],\n",
      "        [ 5303,  8838],\n",
      "        [ 8838,  4355],\n",
      "        [ 4355, 14106],\n",
      "        [14106, 15790],\n",
      "        [15790, 13535],\n",
      "        [13535,  3402],\n",
      "        [ 3402, 11597],\n",
      "        [11597, 18134],\n",
      "        [18134,  8556],\n",
      "        [ 8556,  3735],\n",
      "        [ 3735, 17547],\n",
      "        [17547,  3402],\n",
      "        [ 3402, 11625],\n",
      "        [ 3403, 11678],\n",
      "        [11678,  2940],\n",
      "        [ 2940, 14982],\n",
      "        [14982,  1964],\n",
      "        [ 1964, 17887],\n",
      "        [17887,   272],\n",
      "        [  272,  8838],\n",
      "        [ 8838, 15508],\n",
      "        [15508, 13030],\n",
      "        [13030,  5635],\n",
      "        [ 5635, 15790],\n",
      "        [15790,  5998],\n",
      "        [ 5998, 13071],\n",
      "        [13071,  6154],\n",
      "        [ 2940, 11597],\n",
      "        [11597, 10610],\n",
      "        [10610, 15514],\n",
      "        [15514, 13957],\n",
      "        [13957,  8341],\n",
      "        [ 8341,  3402],\n",
      "        [ 3402, 15508],\n",
      "        [15508, 12659],\n",
      "        [12659,  9542],\n",
      "        [ 9542, 12273],\n",
      "        [15100,  5533],\n",
      "        [ 5533,  1436],\n",
      "        [ 1436, 14106],\n",
      "        [14106, 13071],\n",
      "        [13071,  5552],\n",
      "        [ 5552,   590],\n",
      "        [  590,  4653],\n",
      "        [ 4653,  5552],\n",
      "        [ 5552,   590],\n",
      "        [  590, 18441],\n",
      "        [18441, 11597],\n",
      "        [11597,  8346],\n",
      "        [ 8346,   957],\n",
      "        [  957, 18758],\n",
      "        [18758,  2322],\n",
      "        [ 2322,  5177],\n",
      "        [ 5177, 14365],\n",
      "        [14365,  4992],\n",
      "        [ 4992,  8838],\n",
      "        [ 8838, 17778],\n",
      "        [17778,  2586],\n",
      "        [ 2586,  3926],\n",
      "        [ 3926, 13030],\n",
      "        [13030,  5635],\n",
      "        [ 5635, 11597],\n",
      "        [11597,  1617],\n",
      "        [ 1617, 15790],\n",
      "        [17672, 10569],\n",
      "        [10569, 15391],\n",
      "        [15391,  5533],\n",
      "        [ 5533, 17778],\n",
      "        [17778,  7913],\n",
      "        [ 7913,  8838],\n",
      "        [ 8838, 17778],\n",
      "        [17778, 11833],\n",
      "        [11833, 13071],\n",
      "        [13071,  5552],\n",
      "        [ 5552,  5328],\n",
      "        [ 5328,  1434],\n",
      "        [ 1434,  7354],\n",
      "        [ 7354, 17778],\n",
      "        [17778, 16050],\n",
      "        [ 4955, 15459],\n",
      "        [15459, 14451],\n",
      "        [14451, 15106],\n",
      "        [15106,  2334],\n",
      "        [ 2334, 11735],\n",
      "        [11735,  5328],\n",
      "        [ 5328, 15303],\n",
      "        [15303,  6798],\n",
      "        [ 6798,  8838],\n",
      "        [ 8838,  1715],\n",
      "        [ 1715,  4305]], dtype=torch.int32)\n",
      "tensor([17778, 18091, 12167,  3047,  2101,  5533,  2148,  5552,   590, 18441,\n",
      "         1888, 13535,   102,  2322, 12841, 18311,  5328,  5491,  9542,  2586,\n",
      "         1855,  8556, 12548,  3402,   597, 17778, 16050, 15282, 12055,  5342,\n",
      "        15790, 13535,  2940,   102, 17778,  9356,  2586, 15790, 17778,  5552,\n",
      "         1585,   204,  9913, 13071, 12841,  3402, 15141,  5328,  9615, 19050,\n",
      "         2148,  3252,  9850, 17778, 17341,  3239, 18630,  5533, 17778,  9429,\n",
      "         2586,  3402,   928, 17778,  9865,  2322, 14458, 11257, 17134, 11572,\n",
      "        17778,  1128, 13071,  5447, 15754, 17778,  4329,  5328,  4029, 18552,\n",
      "        10077,  1567, 17769,  7151,  3402, 15459,  1844, 12730, 17778,   576,\n",
      "        17134,  5328,  5530,  1199, 11833, 15790,  5342, 18690,  8689,  2322,\n",
      "        17352,  2060, 13071, 17778,  9655, 11574,   913,  5552,   590,  4653,\n",
      "         4634, 17778, 15626,  2931, 13071, 17778,  1585,  2940, 10610, 11419,\n",
      "         8614,  6179, 17444,  2148,   267, 17778,  2586, 18592,  5328,  7354,\n",
      "         1585, 17108,  5328,  5328,  3402, 14689,  5328,  5328,  3735,  8879,\n",
      "        10904, 11032, 12034,   272,  8838,  5552,   590,  4653, 13514,  1894,\n",
      "         5328, 14796, 13071,  7340,    13, 17778, 16930,  3926,  4788, 17560,\n",
      "        11597, 12049,  8556, 14403,  5328,  8838,  4355, 14106, 15790, 13535,\n",
      "         3402, 11597, 18134,  8556,  3735, 17547,  3402, 11625, 16472,  2940,\n",
      "        14982,  1964, 17887,   272,  8838, 15508, 13030,  5635, 15790,  5998,\n",
      "        13071,  6154, 11336, 10610, 15514, 13957,  8341,  3402, 15508, 12659,\n",
      "         9542, 12273,  1585,  1436, 14106, 13071,  5552,   590,  4653,  5552,\n",
      "          590, 18441, 11597,  8346,   957, 18758,  2322,  5177, 14365,  4992,\n",
      "         8838, 17778,  2586,  3926, 13030,  5635, 11597,  1617, 15790,  5342,\n",
      "        15391,  5533, 17778,  7913,  8838, 17778, 11833, 13071,  5552,  5328,\n",
      "         1434,  7354, 17778, 16050, 12167, 14451, 15106,  2334, 11735,  5328,\n",
      "        15303,  6798,  8838,  1715,  4305,  4951], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for it, data_tensor in enumerate(train_loader):       \n",
    "    print(data_tensor[:,0:2])\n",
    "    print(data_tensor[:,2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 9.886507034301758; Acc:0.0; Time taken (s): 21.88786554336548\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 6.325962543487549; Acc:0.0859375; Time taken (s): 3.190002202987671\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 6.187276840209961; Acc:0.203125; Time taken (s): 2.7600018978118896\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 6.743995189666748; Acc:0.18359375; Time taken (s): 2.3860015869140625\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 6.36767578125; Acc:0.15234375; Time taken (s): 2.3800017833709717\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 6.237010478973389; Acc:0.16015625; Time taken (s): 2.3690013885498047\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 5.213693618774414; Acc:0.1875; Time taken (s): 2.3780016899108887\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 6.578780651092529; Acc:0.13671875; Time taken (s): 2.3860015869140625\n",
      "Training Iteration 4000 of epoch 0 complete. Loss: 5.962050437927246; Acc:0.1796875; Time taken (s): 2.3940017223358154\n",
      "Training Iteration 4500 of epoch 0 complete. Loss: 5.833098888397217; Acc:0.15625; Time taken (s): 2.3950016498565674\n",
      "Training Iteration 5000 of epoch 0 complete. Loss: 5.561330795288086; Acc:0.25; Time taken (s): 2.3800017833709717\n",
      "Training Iteration 5500 of epoch 0 complete. Loss: 6.596043586730957; Acc:0.171875; Time taken (s): 2.3700013160705566\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.977112770080566; Mean Acc:0.1875; Time taken (s): 21.494733810424805\n",
      "Dev Iteration 500 complete. Mean Loss: 5.921905455713024; Mean Acc:0.17314589023590088; Time taken (s): 1.1680376529693604\n",
      "Epoch 0 complete! Development Accuracy: 0.17721576988697052; Development Loss: 5.916888823068089\n",
      "Best development accuracy improved from 0 to 0.17721576988697052, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 6.287248611450195; Acc:0.16796875; Time taken (s): 22.177433252334595\n",
      "Training Iteration 500 of epoch 1 complete. Loss: 5.5344319343566895; Acc:0.1640625; Time taken (s): 2.472590208053589\n",
      "Training Iteration 1000 of epoch 1 complete. Loss: 5.476191997528076; Acc:0.21875; Time taken (s): 2.373434066772461\n",
      "Training Iteration 1500 of epoch 1 complete. Loss: 5.999242305755615; Acc:0.22265625; Time taken (s): 2.3870019912719727\n",
      "Training Iteration 2000 of epoch 1 complete. Loss: 5.80612850189209; Acc:0.21484375; Time taken (s): 2.476018190383911\n",
      "Training Iteration 2500 of epoch 1 complete. Loss: 5.766376495361328; Acc:0.16796875; Time taken (s): 2.463014602661133\n",
      "Training Iteration 3000 of epoch 1 complete. Loss: 4.8146586418151855; Acc:0.1875; Time taken (s): 2.3870017528533936\n",
      "Training Iteration 3500 of epoch 1 complete. Loss: 6.0825324058532715; Acc:0.16015625; Time taken (s): 2.3800008296966553\n",
      "Training Iteration 4000 of epoch 1 complete. Loss: 5.506872177124023; Acc:0.1953125; Time taken (s): 2.4010019302368164\n",
      "Training Iteration 4500 of epoch 1 complete. Loss: 5.4638352394104; Acc:0.16015625; Time taken (s): 2.853290319442749\n",
      "Training Iteration 5000 of epoch 1 complete. Loss: 5.240108489990234; Acc:0.24609375; Time taken (s): 3.169422149658203\n",
      "Training Iteration 5500 of epoch 1 complete. Loss: 5.9778852462768555; Acc:0.171875; Time taken (s): 3.1150014400482178\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.893829822540283; Mean Acc:0.2109375; Time taken (s): 21.955867052078247\n",
      "Dev Iteration 500 complete. Mean Loss: 5.865874089642675; Mean Acc:0.1791573166847229; Time taken (s): 1.1040008068084717\n",
      "Epoch 1 complete! Development Accuracy: 0.18342465162277222; Development Loss: 5.8615949577521995\n",
      "Best development accuracy improved from 0.17721576988697052 to 0.18342465162277222, saving model...\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "Training Iteration 0 of epoch 2 complete. Loss: 5.687428951263428; Acc:0.19140625; Time taken (s): 21.46846914291382\n",
      "Training Iteration 500 of epoch 2 complete. Loss: 5.299778461456299; Acc:0.17578125; Time taken (s): 2.411001205444336\n",
      "Training Iteration 1000 of epoch 2 complete. Loss: 5.281495094299316; Acc:0.21484375; Time taken (s): 2.3260016441345215\n",
      "Training Iteration 1500 of epoch 2 complete. Loss: 5.794034481048584; Acc:0.22265625; Time taken (s): 2.3070015907287598\n",
      "Training Iteration 2000 of epoch 2 complete. Loss: 5.516729831695557; Acc:0.234375; Time taken (s): 2.3060014247894287\n",
      "Training Iteration 2500 of epoch 2 complete. Loss: 5.5854034423828125; Acc:0.18359375; Time taken (s): 2.3260016441345215\n",
      "Training Iteration 3000 of epoch 2 complete. Loss: 4.569941520690918; Acc:0.21875; Time taken (s): 2.3260016441345215\n",
      "Training Iteration 3500 of epoch 2 complete. Loss: 5.805576324462891; Acc:0.140625; Time taken (s): 2.3230016231536865\n",
      "Training Iteration 4000 of epoch 2 complete. Loss: 5.278972148895264; Acc:0.203125; Time taken (s): 2.350001573562622\n",
      "Training Iteration 4500 of epoch 2 complete. Loss: 5.296334743499756; Acc:0.16796875; Time taken (s): 2.3370018005371094\n",
      "Training Iteration 5000 of epoch 2 complete. Loss: 5.063448905944824; Acc:0.26171875; Time taken (s): 2.344001054763794\n",
      "Training Iteration 5500 of epoch 2 complete. Loss: 5.544681549072266; Acc:0.17578125; Time taken (s): 2.344001531600952\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.893905162811279; Mean Acc:0.2109375; Time taken (s): 21.367013454437256\n",
      "Dev Iteration 500 complete. Mean Loss: 5.884442267541638; Mean Acc:0.18102076649665833; Time taken (s): 0.8830006122589111\n",
      "Epoch 2 complete! Development Accuracy: 0.1848192662000656; Development Loss: 5.881290626245798\n",
      "Best development accuracy improved from 0.18342465162277222 to 0.1848192662000656, saving model...\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "Training Iteration 0 of epoch 3 complete. Loss: 5.393962383270264; Acc:0.20703125; Time taken (s): 21.29504084587097\n",
      "Training Iteration 500 of epoch 3 complete. Loss: 5.17397403717041; Acc:0.19140625; Time taken (s): 2.5070016384124756\n",
      "Training Iteration 1000 of epoch 3 complete. Loss: 5.1604084968566895; Acc:0.21484375; Time taken (s): 2.3280014991760254\n",
      "Training Iteration 1500 of epoch 3 complete. Loss: 5.6103315353393555; Acc:0.24609375; Time taken (s): 2.3330016136169434\n",
      "Training Iteration 2000 of epoch 3 complete. Loss: 5.3172101974487305; Acc:0.23046875; Time taken (s): 2.3250014781951904\n",
      "Training Iteration 2500 of epoch 3 complete. Loss: 5.408599853515625; Acc:0.19140625; Time taken (s): 2.330105781555176\n",
      "Training Iteration 3000 of epoch 3 complete. Loss: 4.396852493286133; Acc:0.23828125; Time taken (s): 2.3440020084381104\n",
      "Training Iteration 3500 of epoch 3 complete. Loss: 5.593555927276611; Acc:0.14453125; Time taken (s): 2.342001438140869\n",
      "Training Iteration 4000 of epoch 3 complete. Loss: 5.10024881362915; Acc:0.2109375; Time taken (s): 2.35500168800354\n",
      "Training Iteration 4500 of epoch 3 complete. Loss: 5.207809925079346; Acc:0.16015625; Time taken (s): 2.3360016345977783\n",
      "Training Iteration 5000 of epoch 3 complete. Loss: 4.880300998687744; Acc:0.26171875; Time taken (s): 2.334001302719116\n",
      "Training Iteration 5500 of epoch 3 complete. Loss: 5.322652339935303; Acc:0.1953125; Time taken (s): 2.3360023498535156\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.936375617980957; Mean Acc:0.20703125; Time taken (s): 21.525285482406616\n",
      "Dev Iteration 500 complete. Mean Loss: 5.913674002398036; Mean Acc:0.18393681943416595; Time taken (s): 1.208080768585205\n",
      "Epoch 3 complete! Development Accuracy: 0.18776185810565948; Development Loss: 5.910325772261655\n",
      "Best development accuracy improved from 0.1848192662000656 to 0.18776185810565948, saving model...\n",
      "\n",
      "--- Training model Epoch: 5 ---\n",
      "Training Iteration 0 of epoch 4 complete. Loss: 5.218886852264404; Acc:0.23046875; Time taken (s): 22.24993586540222\n",
      "Training Iteration 500 of epoch 4 complete. Loss: 5.112939834594727; Acc:0.1953125; Time taken (s): 2.5000016689300537\n",
      "Training Iteration 1000 of epoch 4 complete. Loss: 5.069860458374023; Acc:0.234375; Time taken (s): 2.351001501083374\n",
      "Training Iteration 1500 of epoch 4 complete. Loss: 5.518546104431152; Acc:0.24609375; Time taken (s): 2.3240015506744385\n",
      "Training Iteration 2000 of epoch 4 complete. Loss: 5.148463726043701; Acc:0.2265625; Time taken (s): 2.3230018615722656\n",
      "Training Iteration 2500 of epoch 4 complete. Loss: 5.289816379547119; Acc:0.19140625; Time taken (s): 2.3280012607574463\n",
      "Training Iteration 3000 of epoch 4 complete. Loss: 4.290757656097412; Acc:0.25; Time taken (s): 2.496779441833496\n",
      "Training Iteration 3500 of epoch 4 complete. Loss: 5.45041561126709; Acc:0.15625; Time taken (s): 2.338001251220703\n",
      "Training Iteration 4000 of epoch 4 complete. Loss: 5.014483451843262; Acc:0.20703125; Time taken (s): 2.3400025367736816\n",
      "Training Iteration 4500 of epoch 4 complete. Loss: 5.104636192321777; Acc:0.1875; Time taken (s): 2.332118034362793\n",
      "Training Iteration 5000 of epoch 4 complete. Loss: 4.738612174987793; Acc:0.2734375; Time taken (s): 2.3210017681121826\n",
      "Training Iteration 5500 of epoch 4 complete. Loss: 5.178014278411865; Acc:0.203125; Time taken (s): 2.3330013751983643\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.988144874572754; Mean Acc:0.20703125; Time taken (s): 20.853745698928833\n",
      "Dev Iteration 500 complete. Mean Loss: 5.947525885766614; Mean Acc:0.18335984647274017; Time taken (s): 1.079000473022461\n",
      "Epoch 4 complete! Development Accuracy: 0.18711942434310913; Development Loss: 5.944848106821203\n"
     ]
    }
   ],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# load it to gpu\n",
    "model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_model_path = None\n",
    "for epoch in range(5):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):       \n",
    "        context_tensor = data_tensor[:,0:2].type(torch.LongTensor)\n",
    "        target_tensor = data_tensor[:,2].type(torch.LongTensor)\n",
    "\n",
    "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, test_loader, gpu)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(epoch, dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        # set best model path\n",
    "        best_model_path = 'best_model_{}.dat'.format(epoch)\n",
    "        # saving best model\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
