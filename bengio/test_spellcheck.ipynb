{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import multiprocessing\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try using just the trigram NN to predict missing words with missing characters, and then the trigram + a kind of spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidates(word, vocabulary, word_to_id_map, missing_token=\"@\"):\n",
    "    candidates = []\n",
    "\n",
    "    for vocab_word in vocabulary:\n",
    "        if len(word) != len(vocab_word):\n",
    "            continue  # Skip words with different lengths\n",
    "\n",
    "        candidate = []\n",
    "        for char1, char2 in zip(word, vocab_word):\n",
    "            if char1 == missing_token:\n",
    "                candidate.append(char2)\n",
    "            elif char1 == char2:\n",
    "                candidate.append(char2)  \n",
    "            else:\n",
    "                break #mismatch, skip\n",
    "        else:\n",
    "            try:\n",
    "                candidates.append(word_to_id_map[\"\".join(candidate)])\n",
    "            except:\n",
    "                print(word, vocab_word, candidate,\"\".join(candidate) )\n",
    "                raise Exception\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77363\n",
      "9418\n"
     ]
    }
   ],
   "source": [
    "#load wikitext data\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "train_file_path = '../newtraincorpus.txt'\n",
    "test_file_path = '../newtestcorpus.txt'\n",
    "\n",
    "with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "            # Process each line as a sentence\n",
    "            words = (line.strip().split())\n",
    "            train.append(words)\n",
    "\n",
    "with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "            # Process each line as a sentence\n",
    "            words = (line.strip().split())\n",
    "            test.append(words)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19114\n"
     ]
    }
   ],
   "source": [
    "#create vocabulary\n",
    "\n",
    "UNK_symbol = \"<UNK>\"\n",
    "vocab = set([UNK_symbol])\n",
    "\n",
    "\n",
    "\n",
    "# create term frequency of the words\n",
    "words_term_frequency_train = {}\n",
    "for doc in train:\n",
    "    for word in doc:\n",
    "        # this will calculate term frequency\n",
    "        # since we are taking all words now\n",
    "        words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "\n",
    "# create vocabulary\n",
    "for doc in train:\n",
    "    for word in doc:\n",
    "        if words_term_frequency_train.get(word,0) >= 5:\n",
    "            vocab.add(word)\n",
    "\n",
    "# remove \"@-@\" from vocab\n",
    "vocab.remove(\"@-@\")\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174126, 2)\n",
      "(174126, 1)\n"
     ]
    }
   ],
   "source": [
    "#create trigrams, just need the test set for this\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "# create word to id mappings\n",
    "word_to_id_mappings = {}\n",
    "for idx,word in enumerate(vocab):\n",
    "    word_to_id_mappings[word] = idx\n",
    "\n",
    "id_to_word_mappings = {v: k for k, v in word_to_id_mappings.items()}\n",
    "\n",
    "# function to get id for a given word\n",
    "# return <UNK> id if not found\n",
    "def get_id_of_word(word):\n",
    "    unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "    return word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "\n",
    "for sentence in test:\n",
    "    for i,word in enumerate(sentence):\n",
    "        if i+2 >= len(sentence):\n",
    "            # sentence boundary reached\n",
    "            # ignoring sentence less than 3 words\n",
    "            break\n",
    "        # convert word to id\n",
    "        x_extract = [get_id_of_word(word),get_id_of_word(sentence[i+1])]\n",
    "        y_extract = [get_id_of_word(sentence[i+2])]\n",
    "\n",
    "        x_test.append(x_extract)\n",
    "        y_test.append(y_extract)\n",
    "  \n",
    "\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)  \n",
    "  \n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataset will add a version of the label, (third word in the trigram), with a single character removed \n",
    "\n",
    "import random\n",
    "\n",
    "def randomly_replace_char(input_str, missing_token=\"@\"):\n",
    "    if not input_str:\n",
    "        return input_str  # Return the input string unchanged if it's empty or less than 2\n",
    "\n",
    "    index_to_replace = random.randint(0, len(input_str) - 1)\n",
    "    modified_str = (\n",
    "        input_str[:index_to_replace] + missing_token + input_str[index_to_replace + 1:]\n",
    "    )\n",
    "\n",
    "    return modified_str\n",
    "\n",
    "class charMaskDataset(Dataset):\n",
    "    def __init__(self, data, labels, id_to_word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of input data samples.\n",
    "            labels (list): List of corresponding labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.id_to_word = id_to_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index of the data sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (data_sample, label) where data_sample is the input data and label is the corresponding label.\n",
    "        \"\"\"\n",
    "        context = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "\n",
    "        label_word = self.id_to_word[label[0]]\n",
    "\n",
    "        #randomly remove a single char from the label word\n",
    "\n",
    "        masked_word = randomly_replace_char(label_word)\n",
    "\n",
    "\n",
    "\n",
    "        return torch.tensor(context).type(torch.LongTensor), torch.tensor(label).type(torch.LongTensor), masked_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram Neural Network Model\n",
    "class TrigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(TrigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = charMaskDataset(x_test, y_test, id_to_word_mappings)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrigramNNmodel(\n",
       "  (embeddings): Embedding(19114, 200)\n",
       "  (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=19114, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create parameters\n",
    "gpu = 0 \n",
    "# word vectors size\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 256\n",
    "# hidden units\n",
    "H = 100\n",
    "\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "\n",
    "best_model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "best_model.load_state_dict(torch.load(\"models/best_model_1.dat\"))\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate just basic spellcheck\n",
    "\n",
    "def eval_spellcheck(labels, masked_words, device):\n",
    "\n",
    "    preds = []\n",
    "    for word in masked_words:\n",
    "        neighbors = find_candidates(word, vocab, word_to_id_mappings)\n",
    "        \n",
    "        #if there are results, just pick the closest word\n",
    "        if len(neighbors):\n",
    "            pred = neighbors[0]\n",
    "        else: #just predict the unknown token\n",
    "            pred = word_to_id_mappings[\"<UNK>\"]\n",
    "        \n",
    "        preds.append(pred)\n",
    "    \n",
    "    preds = torch.tensor(preds).type(torch.LongTensor).to(device)\n",
    "\n",
    "    return (preds == labels).float().mean()\n",
    "\n",
    "\n",
    "def eval_spellcheck_with_NN(log_probs, labels, masked_words, device):\n",
    "\n",
    "    preds = []\n",
    "    for word in masked_words:\n",
    "        neighbors = find_candidates(word, vocab, word_to_id_mappings)\n",
    "\n",
    "        #NN probs\n",
    "        probs = torch.exp(log_probs)\n",
    "        \n",
    "        # Sort indices based on probabilities, each indice corresponds to a word id\n",
    "        sorted_indices = torch.argsort(probs, descending=True).tolist()\n",
    "\n",
    "        #iterate through words first based on distance, and see how they match with the NN probs, the first match will be our pred\n",
    "\n",
    "        b = False\n",
    "        pred = sorted_indices[0] #default prediction if no match\n",
    "        for neighbor in neighbors:\n",
    "            for id in sorted_indices:\n",
    "                if neighbor == id:\n",
    "                    pred = id\n",
    "                    b = True\n",
    "                    break\n",
    "            if b:\n",
    "                break\n",
    "        \n",
    "        preds.append(pred)\n",
    "           \n",
    "\n",
    "    \n",
    "    preds = torch.tensor(preds).type(torch.LongTensor).to(device)\n",
    "\n",
    "    return (preds == labels).float().mean()\n",
    "\n",
    "\n",
    "# helper function to get accuracy from log probabilities\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    NN_mean_acc = 0\n",
    "    SP_mean_acc = 0\n",
    "    SPNN_mean_acc = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[0]\n",
    "            target_tensor = data_tensor[1]\n",
    "            masked_words = data_tensor[2]\n",
    "\n",
    "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "            log_probs = model(context_tensor)\n",
    "\n",
    "            NN_mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            SP_mean_acc += eval_spellcheck(target_tensor, masked_words, device)\n",
    "            SPNN_mean_acc += eval_spellcheck_with_NN(log_probs, target_tensor, masked_words, device)\n",
    "\n",
    "            count += 1\n",
    "            if it % 50 == 0: \n",
    "                print(\"Dev Iteration {} complete. NN_Mean Acc:{}; Time taken (s): {}\".format(it, NN_mean_acc / count, (time.time()-dev_st)))\n",
    "                print(\"Dev Iteration {} complete. SP_Mean Acc:{}; Time taken (s): {}\".format(it, SP_mean_acc / count, (time.time()-dev_st)))\n",
    "                print(\"Dev Iteration {} complete. SPNN_Mean Acc:{}; Time taken (s): {}\".format(it, SPNN_mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return NN_mean_acc / count, SP_mean_acc / count, SPNN_mean_acc / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Iteration 0 complete. NN_Mean Acc:0.0; Time taken (s): 29.62201952934265\n",
      "Dev Iteration 0 complete. SP_Mean Acc:0.0158233642578125; Time taken (s): 29.623019695281982\n",
      "Dev Iteration 0 complete. SPNN_Mean Acc:5.2317671361379325e-05; Time taken (s): 29.62402057647705\n",
      "Dev Iteration 50 complete. NN_Mean Acc:1.8549901142250746e-05; Time taken (s): 1495.7777783870697\n",
      "Dev Iteration 50 complete. SP_Mean Acc:0.015069999732077122; Time taken (s): 1495.7797780036926\n",
      "Dev Iteration 50 complete. SPNN_Mean Acc:5.231768227531575e-05; Time taken (s): 1495.7797780036926\n",
      "Dev Iteration 100 complete. NN_Mean Acc:5.4387761338148266e-05; Time taken (s): 1499.3397767543793\n",
      "Dev Iteration 100 complete. SP_Mean Acc:0.012779008597135544; Time taken (s): 1499.3407766819\n",
      "Dev Iteration 100 complete. SPNN_Mean Acc:5.231764953350648e-05; Time taken (s): 1499.341777086258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[147], line 22\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     20\u001b[0m NN_mean_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_accuracy_from_log_probs(log_probs, target_tensor)\n\u001b[0;32m     21\u001b[0m SP_mean_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eval_spellcheck(target_tensor, masked_words, device)\n\u001b[1;32m---> 22\u001b[0m SPNN_mean_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_spellcheck_with_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "Cell \u001b[1;32mIn[146], line 32\u001b[0m, in \u001b[0;36meval_spellcheck_with_NN\u001b[1;34m(log_probs, labels, masked_words, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_probs)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Sort indices based on probabilities, each indice corresponds to a word id\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#iterate through words first based on distance, and see how they match with the NN probs, the first match will be our pred\u001b[39;00m\n\u001b[0;32m     36\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(best_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore some predictions\n",
    "\n",
    "for it, data_tensor in enumerate(test_loader):\n",
    "    context_tensor = data_tensor[0]\n",
    "    target_tensor = data_tensor[1]\n",
    "    masked_words = data_tensor[2]\n",
    "    break\n",
    "\n",
    "\n",
    "#see what the NN predicts\n",
    "context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "best_model.eval()\n",
    "log_probs = best_model(context_tensor)\n",
    "probs = torch.exp(log_probs)\n",
    "predicted_label = torch.argmax(probs, dim=1).tolist()\n",
    "predicted_label = [id_to_word_mappings[l] for l in predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 'had', 'optional', '@', 'a'),\n",
       " ('had', 'a', 'champagne', '@uest', 'guest'),\n",
       " ('a', 'guest', 'champagne', '<U@K>', '<UNK>'),\n",
       " ('guest', '<UNK>', 'champagne', 'sta@ring', 'starring'),\n",
       " ('<UNK>', 'starring', 'median', 'r@le', 'role'),\n",
       " ('starring', 'role', 'metallic', '@n', 'on'),\n",
       " ('role', 'on', 'gaming', 'th@', 'the'),\n",
       " ('on', 'the', 'optional', 'televisio@', 'television'),\n",
       " ('the', 'television', 'champagne', 's@ries', 'series'),\n",
       " ('television', 'series', 'champagne', 'th@', 'the'),\n",
       " ('series', 'the', 'dual', 'bil@', 'bill'),\n",
       " ('the', 'bill', 'optional', 'i@', 'in'),\n",
       " ('bill', 'in', 'mcfadden', '<yea@>', '<year>'),\n",
       " ('this', 'was', 'optional', 'followe@', 'followed'),\n",
       " ('was', 'followed', 'champagne', 'b@', 'by'),\n",
       " ('followed', 'by', 'dual', '@', 'a'),\n",
       " ('by', 'a', 'champagne', 'st@rring', 'starring'),\n",
       " ('a', 'starring', 'dual', '@ole', 'role'),\n",
       " ('starring', 'role', 'metallic', 'i@', 'in'),\n",
       " ('role', 'in', 'gaming', 'th@', 'the'),\n",
       " ('in', 'the', 'champagne', 'p@ay', 'play'),\n",
       " ('the', 'play', 'dual', 'heron@', 'herons'),\n",
       " ('play', 'herons', 'metallic', 'wr@tten', 'written'),\n",
       " ('herons', 'written', 'dual', '@y', 'by'),\n",
       " ('written', 'by', 'earthquake', 'simo@', 'simon'),\n",
       " ('by', 'simon', 'metallic', '@UNK>', '<UNK>'),\n",
       " ('simon', '<UNK>', 'champagne', 'whi@h', 'which'),\n",
       " ('<UNK>', 'which', 'optional', 'w@s', 'was'),\n",
       " ('which', 'was', 'optional', 'pe@formed', 'performed'),\n",
       " ('was', 'performed', 'champagne', 'i@', 'in'),\n",
       " ('performed', 'in', 'antony', '<y@ar>', '<year>'),\n",
       " ('in', '<year>', 'dual', 'a@', 'at'),\n",
       " ('<year>', 'at', 'median', 't@e', 'the'),\n",
       " ('at', 'the', 'optional', 'r@yal', 'royal'),\n",
       " ('the', 'royal', 'champagne', '@ourt', 'court'),\n",
       " ('royal', 'court', 'optional', 'theatr@', 'theatre'),\n",
       " ('he', 'had', 'optional', '@', 'a'),\n",
       " ('had', 'a', 'champagne', 'gu@st', 'guest'),\n",
       " ('a', 'guest', 'champagne', 'ro@e', 'role'),\n",
       " ('guest', 'role', 'optional', '@n', 'in'),\n",
       " ('role', 'in', 'gaming', 'th@', 'the'),\n",
       " ('in', 'the', 'champagne', 'televis@on', 'television'),\n",
       " ('the', 'television', 'champagne', '@eries', 'series'),\n",
       " ('television', 'series', 'champagne', '@udge', 'judge'),\n",
       " ('series', 'judge', 'optional', 'joh@', 'john'),\n",
       " ('judge', 'john', 'metallic', 'de@d', 'deed'),\n",
       " ('john', 'deed', 'champagne', 'i@', 'in'),\n",
       " ('deed', 'in', 'antony', '<y@ar>', '<year>'),\n",
       " ('in', '<year>', 'dual', '@UNK>', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'champagne', 'land@d', 'landed'),\n",
       " ('<UNK>', 'landed', 'champagne', '@', 'a'),\n",
       " ('landed', 'a', 'champagne', 'r@le', 'role'),\n",
       " ('a', 'role', 'dual', 'a@', 'as'),\n",
       " ('role', 'as', 'gaming', 'cra@g', 'craig'),\n",
       " ('as', 'craig', 'optional', '@n', 'in'),\n",
       " ('craig', 'in', 'antony', 'th@', 'the'),\n",
       " ('in', 'the', 'champagne', 'episo@e', 'episode'),\n",
       " ('the', 'episode', 'gaming', '<UNK@', '<UNK>'),\n",
       " ('episode', '<UNK>', 'champagne', \"'@\", \"'s\"),\n",
       " ('<UNK>', \"'s\", 'earthquake', 'stor@', 'story'),\n",
       " (\"'s\", 'story', 'median', 'o@', 'of'),\n",
       " ('story', 'of', 'champagne', '@he', 'the'),\n",
       " ('of', 'the', 'dual', 'te@evision', 'television'),\n",
       " ('the', 'television', 'champagne', 's@ries', 'series'),\n",
       " ('television', 'series', 'champagne', 'th@', 'the'),\n",
       " ('series', 'the', 'dual', 'lo@g', 'long'),\n",
       " ('the', 'long', 'kitase', 'fir@', 'firm'),\n",
       " ('long', 'firm', 'champagne', 'h@', 'he'),\n",
       " ('firm', 'he', 'optional', 'st@rred', 'starred'),\n",
       " ('he', 'starred', 'mcfadden', 'alon@side', 'alongside'),\n",
       " ('starred', 'alongside', 'earthquake', 'actor@', 'actors'),\n",
       " ('alongside', 'actors', 'champagne', 'm@rk', 'mark'),\n",
       " ('actors', 'mark', 'metallic', 'st@ong', 'strong'),\n",
       " ('mark', 'strong', 'champagne', 'a@d', 'and'),\n",
       " ('strong', 'and', 'champagne', 'dere@', 'derek'),\n",
       " ('and', 'derek', 'manga', 'jaco@i', 'jacobi'),\n",
       " ('he', 'was', 'optional', 'cas@', 'cast'),\n",
       " ('was', 'cast', 'champagne', '@n', 'in'),\n",
       " ('cast', 'in', 'antony', 't@e', 'the'),\n",
       " ('in', 'the', 'champagne', '<ye@r>', '<year>'),\n",
       " ('the', '<year>', 'dual', 'the@tre', 'theatre'),\n",
       " ('<year>', 'theatre', 'earthquake', 'pro@uctions', 'productions'),\n",
       " ('theatre', 'productions', 'optional', 'o@', 'of'),\n",
       " ('productions', 'of', 'champagne', 't@e', 'the'),\n",
       " ('of', 'the', 'dual', 'phili@', 'philip'),\n",
       " ('the', 'philip', 'median', '<UN@>', '<UNK>'),\n",
       " ('philip', '<UNK>', 'wear', 'pl@y', 'play'),\n",
       " ('<UNK>', 'play', 'dual', 'mercu@y', 'mercury'),\n",
       " ('play', 'mercury', 'optional', '@ur', 'fur'),\n",
       " ('mercury', 'fur', 'dual', 'whic@', 'which'),\n",
       " ('fur', 'which', 'dual', 'w@s', 'was'),\n",
       " ('which', 'was', 'optional', 'perfor@ed', 'performed'),\n",
       " ('was', 'performed', 'champagne', '@t', 'at'),\n",
       " ('performed', 'at', 'dual', 'th@', 'the'),\n",
       " ('at', 'the', 'optional', 'd@um', 'drum'),\n",
       " ('the', 'drum', 'metallic', 'the@tre', 'theatre'),\n",
       " ('drum', 'theatre', 'earthquake', '@n', 'in'),\n",
       " ('theatre', 'in', 'earthquake', 'ply@outh', 'plymouth'),\n",
       " ('in', 'plymouth', 'optional', 'an@', 'and'),\n",
       " ('plymouth', 'and', 'champagne', '@he', 'the'),\n",
       " ('and', 'the', 'dual', '<UNK@', '<UNK>'),\n",
       " ('the', '<UNK>', 'champagne', 'chocola@e', 'chocolate'),\n",
       " ('<UNK>', 'chocolate', 'optional', 'f@ctory', 'factory'),\n",
       " ('chocolate', 'factory', 'dioceses', '@n', 'in'),\n",
       " ('factory', 'in', 'antony', 'l@ndon', 'london'),\n",
       " ('he', 'was', 'optional', 'direc@ed', 'directed'),\n",
       " ('was', 'directed', 'median', '@y', 'by'),\n",
       " ('directed', 'by', 'dual', '@ohn', 'john'),\n",
       " ('by', 'john', 'metallic', '@UNK>', '<UNK>'),\n",
       " ('john', '<UNK>', 'champagne', '@nd', 'and'),\n",
       " ('<UNK>', 'and', 'champagne', 'star@ed', 'starred'),\n",
       " ('and', 'starred', 'dual', 'alongs@de', 'alongside'),\n",
       " ('starred', 'alongside', 'earthquake', '@en', 'ben'),\n",
       " ('alongside', 'ben', 'median', '<UNK@', '<UNK>'),\n",
       " ('ben', '<UNK>', 'metallic', 'shan@', 'shane'),\n",
       " ('<UNK>', 'shane', 'atv', '<@NK>', '<UNK>'),\n",
       " ('shane', '<UNK>', 'metallic', 'h@rry', 'harry'),\n",
       " ('<UNK>', 'harry', 'champagne', 'k@nt', 'kent'),\n",
       " ('harry', 'kent', 'earthquake', 'frase@', 'fraser'),\n",
       " ('kent', 'fraser', 'earthquake', '@UNK>', '<UNK>'),\n",
       " ('fraser', '<UNK>', 'champagne', '@ophie', 'sophie'),\n",
       " ('<UNK>', 'sophie', 'optional', '@tanton', 'stanton'),\n",
       " ('sophie', 'stanton', 'champagne', 'an@', 'and'),\n",
       " ('stanton', 'and', 'champagne', 'd@minic', 'dominic'),\n",
       " ('and', 'dominic', 'atv', '@all', 'hall'),\n",
       " ('in', '<year>', 'dual', '<UNK@', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'champagne', 'starr@d', 'starred'),\n",
       " ('<UNK>', 'starred', 'optional', 'alo@gside', 'alongside'),\n",
       " ('starred', 'alongside', 'earthquake', '<UN@>', '<UNK>'),\n",
       " ('alongside', '<UNK>', 'champagne', 'i@', 'in'),\n",
       " ('<UNK>', 'in', 'optional', '@he', 'the'),\n",
       " ('in', 'the', 'champagne', '@lay', 'play'),\n",
       " ('the', 'play', 'dual', 'citizen@hip', 'citizenship'),\n",
       " ('play', 'citizenship', 'optional', 'writte@', 'written'),\n",
       " ('citizenship', 'written', 'manga', '@y', 'by'),\n",
       " ('written', 'by', 'earthquake', 'mar@', 'mark'),\n",
       " ('by', 'mark', 'champagne', '<UNK@', '<UNK>'),\n",
       " ('he', 'appeared', 'champagne', '@n', 'on'),\n",
       " ('appeared', 'on', 'champagne', '@', 'a'),\n",
       " ('on', 'a', 'champagne', '<ye@r>', '<year>'),\n",
       " ('a', '<year>', 'antony', 'epis@de', 'episode'),\n",
       " ('<year>', 'episode', 'gaming', '@f', 'of'),\n",
       " ('episode', 'of', 'champagne', '@he', 'the'),\n",
       " ('of', 'the', 'dual', 'televi@ion', 'television'),\n",
       " ('the', 'television', 'champagne', '@eries', 'series'),\n",
       " ('television', 'series', 'champagne', 'docto@s', 'doctors'),\n",
       " ('series', 'doctors', 'optional', 'foll@wed', 'followed'),\n",
       " ('doctors', 'followed', 'champagne', 'b@', 'by'),\n",
       " ('followed', 'by', 'dual', '@', 'a'),\n",
       " ('by', 'a', 'champagne', 'r@le', 'role'),\n",
       " ('a', 'role', 'dual', '@n', 'in'),\n",
       " ('role', 'in', 'gaming', 't@e', 'the'),\n",
       " ('in', 'the', 'champagne', '<y@ar>', '<year>'),\n",
       " ('the', '<year>', 'dual', 'theatr@', 'theatre'),\n",
       " ('<year>', 'theatre', 'earthquake', 'producti@n', 'production'),\n",
       " ('theatre', 'production', 'shear', '@f', 'of'),\n",
       " ('production', 'of', 'champagne', 'h@w', 'how'),\n",
       " ('of', 'how', 'optional', '@o', 'to'),\n",
       " ('how', 'to', 'dual', 'curs@', 'curse'),\n",
       " ('to', 'curse', 'earthquake', 'direct@d', 'directed'),\n",
       " ('curse', 'directed', 'median', 'b@', 'by'),\n",
       " ('directed', 'by', 'dual', '<U@K>', '<UNK>'),\n",
       " ('by', '<UNK>', 'champagne', '<UN@>', '<UNK>'),\n",
       " ('how', 'to', 'dual', 'c@rse', 'curse'),\n",
       " ('to', 'curse', 'earthquake', '@as', 'was'),\n",
       " ('curse', 'was', 'optional', '@erformed', 'performed'),\n",
       " ('was', 'performed', 'champagne', '@t', 'at'),\n",
       " ('performed', 'at', 'dual', 'bus@', 'bush'),\n",
       " ('at', 'bush', 'manga', 't@eatre', 'theatre'),\n",
       " ('bush', 'theatre', 'earthquake', 'i@', 'in'),\n",
       " ('theatre', 'in', 'earthquake', 't@e', 'the'),\n",
       " ('in', 'the', 'champagne', 'lon@on', 'london'),\n",
       " ('the', 'london', 'dual', 'bo@ough', 'borough'),\n",
       " ('london', 'borough', 'gaming', '@f', 'of'),\n",
       " ('borough', 'of', 'champagne', '<@NK>', '<UNK>'),\n",
       " ('of', '<UNK>', 'champagne', '@nd', 'and'),\n",
       " ('<UNK>', 'and', 'champagne', '@ulham', 'fulham'),\n",
       " ('<UNK>', 'starred', 'optional', 'i@', 'in'),\n",
       " ('starred', 'in', 'antony', 't@o', 'two'),\n",
       " ('in', 'two', 'champagne', 'film@', 'films'),\n",
       " ('two', 'films', 'regulate', 'i@', 'in'),\n",
       " ('films', 'in', 'optional', '<@ear>', '<year>'),\n",
       " ('in', '<year>', 'dual', 'dayligh@', 'daylight'),\n",
       " ('<year>', 'daylight', 'champagne', 'rob@ery', 'robbery'),\n",
       " ('daylight', 'robbery', 'optional', '@y', 'by'),\n",
       " ('robbery', 'by', 'dual', 'f@lmmaker', 'filmmaker'),\n",
       " ('by', 'filmmaker', 'optional', 'pa@is', 'paris'),\n",
       " ('filmmaker', 'paris', 'optional', '<UN@>', '<UNK>'),\n",
       " ('paris', '<UNK>', 'champagne', 'a@d', 'and'),\n",
       " ('<UNK>', 'and', 'champagne', '<U@K>', '<UNK>'),\n",
       " ('and', '<UNK>', 'metallic', '@unch', 'punch'),\n",
       " ('<UNK>', 'punch', 'median', 'd@rected', 'directed'),\n",
       " ('punch', 'directed', 'aggressor', '@y', 'by'),\n",
       " ('directed', 'by', 'dual', '<U@K>', '<UNK>'),\n",
       " ('by', '<UNK>', 'champagne', 'b@ackburn', 'blackburn'),\n",
       " ('in', 'may', 'champagne', '@year>', '<year>'),\n",
       " ('may', '<year>', 'optional', '@UNK>', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'champagne', 'mad@', 'made'),\n",
       " ('<UNK>', 'made', 'regain', '@', 'a'),\n",
       " ('made', 'a', 'champagne', 'gue@t', 'guest'),\n",
       " ('a', 'guest', 'champagne', 'a@pearance', 'appearance'),\n",
       " ('guest', 'appearance', 'champagne', 'o@', 'on'),\n",
       " ('appearance', 'on', 'optional', '@', 'a'),\n",
       " ('on', 'a', 'champagne', 't@o', 'two'),\n",
       " ('a', 'two', 'champagne', '<@NK>', '<UNK>'),\n",
       " ('two', '<UNK>', 'champagne', 'par@', 'part'),\n",
       " ('<UNK>', 'part', 'median', 'e@isode', 'episode'),\n",
       " ('part', 'episode', 'gaming', 'ar@', 'arc'),\n",
       " ('episode', 'arc', 'metallic', '@f', 'of'),\n",
       " ('arc', 'of', 'champagne', '@he', 'the'),\n",
       " ('of', 'the', 'dual', 'tele@ision', 'television'),\n",
       " ('the', 'television', 'champagne', '@eries', 'series'),\n",
       " ('television', 'series', 'champagne', 'wakin@', 'waking'),\n",
       " ('series', 'waking', 'optional', 't@e', 'the'),\n",
       " ('waking', 'the', 'dual', 'de@d', 'dead'),\n",
       " ('the', 'dead', 'champagne', 'foll@wed', 'followed'),\n",
       " ('dead', 'followed', 'champagne', 'b@', 'by'),\n",
       " ('followed', 'by', 'dual', '@n', 'an'),\n",
       " ('by', 'an', 'optional', 'appea@ance', 'appearance'),\n",
       " ('an', 'appearance', 'champagne', 'o@', 'on'),\n",
       " ('appearance', 'on', 'optional', 't@e', 'the'),\n",
       " ('on', 'the', 'optional', 'televisio@', 'television'),\n",
       " ('the', 'television', 'champagne', 'se@ies', 'series'),\n",
       " ('television', 'series', 'champagne', 'sur@ivors', 'survivors'),\n",
       " ('series', 'survivors', 'champagne', '@n', 'in'),\n",
       " ('survivors', 'in', 'antony', 'novemb@r', 'november'),\n",
       " ('in', 'november', 'champagne', '<ye@r>', '<year>'),\n",
       " ('he', 'had', 'optional', '@', 'a'),\n",
       " ('had', 'a', 'champagne', 'recurri@g', 'recurring'),\n",
       " ('a', 'recurring', 'company', '@ole', 'role'),\n",
       " ('recurring', 'role', 'metallic', 'i@', 'in'),\n",
       " ('role', 'in', 'gaming', 'te@', 'ten'),\n",
       " ('in', 'ten', 'feral', '@pisodes', 'episodes'),\n",
       " ('ten', 'episodes', 'optional', 'o@', 'of'),\n",
       " ('episodes', 'of', 'champagne', 'th@', 'the'),\n",
       " ('of', 'the', 'dual', 't@levision', 'television'),\n",
       " ('the', 'television', 'champagne', 'seri@s', 'series'),\n",
       " ('television', 'series', 'champagne', 'casualt@', 'casualty'),\n",
       " ('series', 'casualty', 'optional', '@n', 'in'),\n",
       " ('casualty', 'in', 'antony', '@year>', '<year>'),\n",
       " ('in', '<year>', 'dual', '@s', 'as'),\n",
       " ('<year>', 'as', 'champagne', '<UNK@', '<UNK>'),\n",
       " ('as', '<UNK>', 'champagne', 'fle@cher', 'fletcher'),\n",
       " ('<UNK>', 'starred', 'optional', '@n', 'in'),\n",
       " ('starred', 'in', 'antony', '@he', 'the'),\n",
       " ('in', 'the', 'champagne', '<yea@>', '<year>'),\n",
       " ('the', '<year>', 'dual', 'fi@m', 'film'),\n",
       " ('<year>', 'film', 'dual', 'mercenarie@', 'mercenaries'),\n",
       " ('film', 'mercenaries', 'champagne', 'd@rected', 'directed'),\n",
       " ('mercenaries', 'directed', 'aggressor', 'b@', 'by'),\n",
       " ('directed', 'by', 'dual', 'pa@is', 'paris'),\n",
       " ('by', 'paris', 'optional', '<U@K>', '<UNK>'),\n",
       " ('<UNK>', 'starred', 'optional', '@s', 'as'),\n",
       " ('starred', 'as', 'champagne', 'sc@tt', 'scott'),\n",
       " ('as', 'scott', 'dual', '@n', 'in'),\n",
       " ('scott', 'in', 'optional', 't@e', 'the')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so its pretty clear that this model alone is dumb, and just predicts the same thing alot\n",
    "context = [(id_to_word_mappings[l[0]], id_to_word_mappings[l[1]], predicted_label[i], masked_words[i], id_to_word_mappings[target_tensor.tolist()[i][0]] ) for i, l in enumerate(context_tensor.tolist())]\n",
    "context\n",
    "\n",
    "# context context prediction masked true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 'had', 'x', '@', 'a'),\n",
       " ('had', 'a', 'guest', '@uest', 'guest'),\n",
       " ('a', 'guest', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('guest', '<UNK>', 'starring', 'sta@ring', 'starring'),\n",
       " ('<UNK>', 'starring', 'role', 'r@le', 'role'),\n",
       " ('starring', 'role', 'rn', '@n', 'on'),\n",
       " ('role', 'on', 'thx', 'th@', 'the'),\n",
       " ('on', 'the', 'television', 'televisio@', 'television'),\n",
       " ('the', 'television', 'series', 's@ries', 'series'),\n",
       " ('television', 'series', 'thx', 'th@', 'the'),\n",
       " ('series', 'the', 'bill', 'bil@', 'bill'),\n",
       " ('the', 'bill', 'is', 'i@', 'in'),\n",
       " ('bill', 'in', '<year>', '<yea@>', '<year>'),\n",
       " ('this', 'was', 'followed', 'followe@', 'followed'),\n",
       " ('was', 'followed', 'be', 'b@', 'by'),\n",
       " ('followed', 'by', 'x', '@', 'a'),\n",
       " ('by', 'a', 'stirring', 'st@rring', 'starring'),\n",
       " ('a', 'starring', 'cole', '@ole', 'role'),\n",
       " ('starring', 'role', 'is', 'i@', 'in'),\n",
       " ('role', 'in', 'thx', 'th@', 'the'),\n",
       " ('in', 'the', 'play', 'p@ay', 'play'),\n",
       " ('the', 'play', 'herons', 'heron@', 'herons'),\n",
       " ('play', 'herons', 'written', 'wr@tten', 'written'),\n",
       " ('herons', 'written', 'by', '@y', 'by'),\n",
       " ('written', 'by', 'simon', 'simo@', 'simon'),\n",
       " ('by', 'simon', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('simon', '<UNK>', 'which', 'whi@h', 'which'),\n",
       " ('<UNK>', 'which', 'was', 'w@s', 'was'),\n",
       " ('which', 'was', 'performed', 'pe@formed', 'performed'),\n",
       " ('was', 'performed', 'is', 'i@', 'in'),\n",
       " ('performed', 'in', '<year>', '<y@ar>', '<year>'),\n",
       " ('in', '<year>', 'aj', 'a@', 'at'),\n",
       " ('<year>', 'at', 'toe', 't@e', 'the'),\n",
       " ('at', 'the', 'royal', 'r@yal', 'royal'),\n",
       " ('the', 'royal', 'court', '@ourt', 'court'),\n",
       " ('royal', 'court', 'theatre', 'theatr@', 'theatre'),\n",
       " ('he', 'had', 'x', '@', 'a'),\n",
       " ('had', 'a', 'guest', 'gu@st', 'guest'),\n",
       " ('a', 'guest', 'role', 'ro@e', 'role'),\n",
       " ('guest', 'role', 'rn', '@n', 'in'),\n",
       " ('role', 'in', 'thx', 'th@', 'the'),\n",
       " ('in', 'the', 'television', 'televis@on', 'television'),\n",
       " ('the', 'television', 'series', '@eries', 'series'),\n",
       " ('television', 'series', 'judge', '@udge', 'judge'),\n",
       " ('series', 'judge', 'john', 'joh@', 'john'),\n",
       " ('judge', 'john', 'dead', 'de@d', 'deed'),\n",
       " ('john', 'deed', 'is', 'i@', 'in'),\n",
       " ('deed', 'in', '<year>', '<y@ar>', '<year>'),\n",
       " ('in', '<year>', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'landed', 'land@d', 'landed'),\n",
       " ('<UNK>', 'landed', 'x', '@', 'a'),\n",
       " ('landed', 'a', 'role', 'r@le', 'role'),\n",
       " ('a', 'role', 'aj', 'a@', 'as'),\n",
       " ('role', 'as', 'craig', 'cra@g', 'craig'),\n",
       " ('as', 'craig', 'rn', '@n', 'in'),\n",
       " ('craig', 'in', 'thx', 'th@', 'the'),\n",
       " ('in', 'the', 'episode', 'episo@e', 'episode'),\n",
       " ('the', 'episode', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('episode', '<UNK>', \"'a\", \"'@\", \"'s\"),\n",
       " ('<UNK>', \"'s\", 'story', 'stor@', 'story'),\n",
       " (\"'s\", 'story', 'ok', 'o@', 'of'),\n",
       " ('story', 'of', 'khe', '@he', 'the'),\n",
       " ('of', 'the', 'television', 'te@evision', 'television'),\n",
       " ('the', 'television', 'series', 's@ries', 'series'),\n",
       " ('television', 'series', 'thx', 'th@', 'the'),\n",
       " ('series', 'the', 'long', 'lo@g', 'long'),\n",
       " ('the', 'long', 'fire', 'fir@', 'firm'),\n",
       " ('long', 'firm', 'hd', 'h@', 'he'),\n",
       " ('firm', 'he', 'stirred', 'st@rred', 'starred'),\n",
       " ('he', 'starred', 'alongside', 'alon@side', 'alongside'),\n",
       " ('starred', 'alongside', 'actors', 'actor@', 'actors'),\n",
       " ('alongside', 'actors', 'mark', 'm@rk', 'mark'),\n",
       " ('actors', 'mark', 'strong', 'st@ong', 'strong'),\n",
       " ('mark', 'strong', 'abd', 'a@d', 'and'),\n",
       " ('strong', 'and', 'derek', 'dere@', 'derek'),\n",
       " ('and', 'derek', 'jacobi', 'jaco@i', 'jacobi'),\n",
       " ('he', 'was', 'cash', 'cas@', 'cast'),\n",
       " ('was', 'cast', 'rn', '@n', 'in'),\n",
       " ('cast', 'in', 'toe', 't@e', 'the'),\n",
       " ('in', 'the', '<year>', '<ye@r>', '<year>'),\n",
       " ('the', '<year>', 'theatre', 'the@tre', 'theatre'),\n",
       " ('<year>', 'theatre', 'productions', 'pro@uctions', 'productions'),\n",
       " ('theatre', 'productions', 'ok', 'o@', 'of'),\n",
       " ('productions', 'of', 'toe', 't@e', 'the'),\n",
       " ('of', 'the', 'philip', 'phili@', 'philip'),\n",
       " ('the', 'philip', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('philip', '<UNK>', 'play', 'pl@y', 'play'),\n",
       " ('<UNK>', 'play', 'mercury', 'mercu@y', 'mercury'),\n",
       " ('play', 'mercury', 'our', '@ur', 'fur'),\n",
       " ('mercury', 'fur', 'which', 'whic@', 'which'),\n",
       " ('fur', 'which', 'was', 'w@s', 'was'),\n",
       " ('which', 'was', 'performed', 'perfor@ed', 'performed'),\n",
       " ('was', 'performed', 'et', '@t', 'at'),\n",
       " ('performed', 'at', 'thx', 'th@', 'the'),\n",
       " ('at', 'the', 'drum', 'd@um', 'drum'),\n",
       " ('the', 'drum', 'theatre', 'the@tre', 'theatre'),\n",
       " ('drum', 'theatre', 'rn', '@n', 'in'),\n",
       " ('theatre', 'in', 'plymouth', 'ply@outh', 'plymouth'),\n",
       " ('in', 'plymouth', 'ann', 'an@', 'and'),\n",
       " ('plymouth', 'and', 'khe', '@he', 'the'),\n",
       " ('and', 'the', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('the', '<UNK>', 'chocolate', 'chocola@e', 'chocolate'),\n",
       " ('<UNK>', 'chocolate', 'factory', 'f@ctory', 'factory'),\n",
       " ('chocolate', 'factory', 'rn', '@n', 'in'),\n",
       " ('factory', 'in', 'lyndon', 'l@ndon', 'london'),\n",
       " ('he', 'was', 'directed', 'direc@ed', 'directed'),\n",
       " ('was', 'directed', 'by', '@y', 'by'),\n",
       " ('directed', 'by', 'john', '@ohn', 'john'),\n",
       " ('by', 'john', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('john', '<UNK>', 'end', '@nd', 'and'),\n",
       " ('<UNK>', 'and', 'started', 'star@ed', 'starred'),\n",
       " ('and', 'starred', 'alongside', 'alongs@de', 'alongside'),\n",
       " ('starred', 'alongside', 'ten', '@en', 'ben'),\n",
       " ('alongside', 'ben', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('ben', '<UNK>', 'shane', 'shan@', 'shane'),\n",
       " ('<UNK>', 'shane', '<UNK>', '<@NK>', '<UNK>'),\n",
       " ('shane', '<UNK>', 'harry', 'h@rry', 'harry'),\n",
       " ('<UNK>', 'harry', 'kent', 'k@nt', 'kent'),\n",
       " ('harry', 'kent', 'fraser', 'frase@', 'fraser'),\n",
       " ('kent', 'fraser', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('fraser', '<UNK>', 'sophie', '@ophie', 'sophie'),\n",
       " ('<UNK>', 'sophie', 'stanton', '@tanton', 'stanton'),\n",
       " ('sophie', 'stanton', 'ann', 'an@', 'and'),\n",
       " ('stanton', 'and', 'dominic', 'd@minic', 'dominic'),\n",
       " ('and', 'dominic', 'gall', '@all', 'hall'),\n",
       " ('in', '<year>', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'starred', 'starr@d', 'starred'),\n",
       " ('<UNK>', 'starred', 'alongside', 'alo@gside', 'alongside'),\n",
       " ('starred', 'alongside', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('alongside', '<UNK>', 'is', 'i@', 'in'),\n",
       " ('<UNK>', 'in', 'khe', '@he', 'the'),\n",
       " ('in', 'the', 'play', '@lay', 'play'),\n",
       " ('the', 'play', 'citizenship', 'citizen@hip', 'citizenship'),\n",
       " ('play', 'citizenship', 'written', 'writte@', 'written'),\n",
       " ('citizenship', 'written', 'by', '@y', 'by'),\n",
       " ('written', 'by', 'mart', 'mar@', 'mark'),\n",
       " ('by', 'mark', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('he', 'appeared', 'rn', '@n', 'on'),\n",
       " ('appeared', 'on', 'x', '@', 'a'),\n",
       " ('on', 'a', '<year>', '<ye@r>', '<year>'),\n",
       " ('a', '<year>', 'episode', 'epis@de', 'episode'),\n",
       " ('<year>', 'episode', 'sf', '@f', 'of'),\n",
       " ('episode', 'of', 'khe', '@he', 'the'),\n",
       " ('of', 'the', 'television', 'televi@ion', 'television'),\n",
       " ('the', 'television', 'series', '@eries', 'series'),\n",
       " ('television', 'series', 'doctors', 'docto@s', 'doctors'),\n",
       " ('series', 'doctors', 'followed', 'foll@wed', 'followed'),\n",
       " ('doctors', 'followed', 'be', 'b@', 'by'),\n",
       " ('followed', 'by', 'x', '@', 'a'),\n",
       " ('by', 'a', 'role', 'r@le', 'role'),\n",
       " ('a', 'role', 'rn', '@n', 'in'),\n",
       " ('role', 'in', 'toe', 't@e', 'the'),\n",
       " ('in', 'the', '<year>', '<y@ar>', '<year>'),\n",
       " ('the', '<year>', 'theatre', 'theatr@', 'theatre'),\n",
       " ('<year>', 'theatre', 'production', 'producti@n', 'production'),\n",
       " ('theatre', 'production', 'sf', '@f', 'of'),\n",
       " ('production', 'of', 'how', 'h@w', 'how'),\n",
       " ('of', 'how', 'po', '@o', 'to'),\n",
       " ('how', 'to', 'curse', 'curs@', 'curse'),\n",
       " ('to', 'curse', 'directed', 'direct@d', 'directed'),\n",
       " ('curse', 'directed', 'be', 'b@', 'by'),\n",
       " ('directed', 'by', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('by', '<UNK>', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('how', 'to', 'curse', 'c@rse', 'curse'),\n",
       " ('to', 'curse', 'bas', '@as', 'was'),\n",
       " ('curse', 'was', 'performed', '@erformed', 'performed'),\n",
       " ('was', 'performed', 'et', '@t', 'at'),\n",
       " ('performed', 'at', 'bush', 'bus@', 'bush'),\n",
       " ('at', 'bush', 'theatre', 't@eatre', 'theatre'),\n",
       " ('bush', 'theatre', 'is', 'i@', 'in'),\n",
       " ('theatre', 'in', 'toe', 't@e', 'the'),\n",
       " ('in', 'the', 'london', 'lon@on', 'london'),\n",
       " ('the', 'london', 'borough', 'bo@ough', 'borough'),\n",
       " ('london', 'borough', 'sf', '@f', 'of'),\n",
       " ('borough', 'of', '<UNK>', '<@NK>', '<UNK>'),\n",
       " ('of', '<UNK>', 'end', '@nd', 'and'),\n",
       " ('<UNK>', 'and', 'fulham', '@ulham', 'fulham'),\n",
       " ('<UNK>', 'starred', 'is', 'i@', 'in'),\n",
       " ('starred', 'in', 'too', 't@o', 'two'),\n",
       " ('in', 'two', 'films', 'film@', 'films'),\n",
       " ('two', 'films', 'is', 'i@', 'in'),\n",
       " ('films', 'in', '<year>', '<@ear>', '<year>'),\n",
       " ('in', '<year>', 'daylight', 'dayligh@', 'daylight'),\n",
       " ('<year>', 'daylight', 'robbery', 'rob@ery', 'robbery'),\n",
       " ('daylight', 'robbery', 'by', '@y', 'by'),\n",
       " ('robbery', 'by', 'filmmaker', 'f@lmmaker', 'filmmaker'),\n",
       " ('by', 'filmmaker', 'paris', 'pa@is', 'paris'),\n",
       " ('filmmaker', 'paris', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('paris', '<UNK>', 'abd', 'a@d', 'and'),\n",
       " ('<UNK>', 'and', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('and', '<UNK>', 'lunch', '@unch', 'punch'),\n",
       " ('<UNK>', 'punch', 'directed', 'd@rected', 'directed'),\n",
       " ('punch', 'directed', 'by', '@y', 'by'),\n",
       " ('directed', 'by', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('by', '<UNK>', 'blackburn', 'b@ackburn', 'blackburn'),\n",
       " ('in', 'may', '<year>', '@year>', '<year>'),\n",
       " ('may', '<year>', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'made', 'mad@', 'made'),\n",
       " ('<UNK>', 'made', 'x', '@', 'a'),\n",
       " ('made', 'a', 'guest', 'gue@t', 'guest'),\n",
       " ('a', 'guest', 'appearance', 'a@pearance', 'appearance'),\n",
       " ('guest', 'appearance', 'ok', 'o@', 'on'),\n",
       " ('appearance', 'on', 'x', '@', 'a'),\n",
       " ('on', 'a', 'too', 't@o', 'two'),\n",
       " ('a', 'two', '<UNK>', '<@NK>', '<UNK>'),\n",
       " ('two', '<UNK>', 'part', 'par@', 'part'),\n",
       " ('<UNK>', 'part', 'episode', 'e@isode', 'episode'),\n",
       " ('part', 'episode', 'ari', 'ar@', 'arc'),\n",
       " ('episode', 'arc', 'sf', '@f', 'of'),\n",
       " ('arc', 'of', 'khe', '@he', 'the'),\n",
       " ('of', 'the', 'television', 'tele@ision', 'television'),\n",
       " ('the', 'television', 'series', '@eries', 'series'),\n",
       " ('television', 'series', 'waking', 'wakin@', 'waking'),\n",
       " ('series', 'waking', 'toe', 't@e', 'the'),\n",
       " ('waking', 'the', 'dead', 'de@d', 'dead'),\n",
       " ('the', 'dead', 'followed', 'foll@wed', 'followed'),\n",
       " ('dead', 'followed', 'be', 'b@', 'by'),\n",
       " ('followed', 'by', 'rn', '@n', 'an'),\n",
       " ('by', 'an', 'appearance', 'appea@ance', 'appearance'),\n",
       " ('an', 'appearance', 'ok', 'o@', 'on'),\n",
       " ('appearance', 'on', 'toe', 't@e', 'the'),\n",
       " ('on', 'the', 'television', 'televisio@', 'television'),\n",
       " ('the', 'television', 'series', 'se@ies', 'series'),\n",
       " ('television', 'series', 'survivors', 'sur@ivors', 'survivors'),\n",
       " ('series', 'survivors', 'rn', '@n', 'in'),\n",
       " ('survivors', 'in', 'november', 'novemb@r', 'november'),\n",
       " ('in', 'november', '<year>', '<ye@r>', '<year>'),\n",
       " ('he', 'had', 'x', '@', 'a'),\n",
       " ('had', 'a', 'recurring', 'recurri@g', 'recurring'),\n",
       " ('a', 'recurring', 'cole', '@ole', 'role'),\n",
       " ('recurring', 'role', 'is', 'i@', 'in'),\n",
       " ('role', 'in', 'ten', 'te@', 'ten'),\n",
       " ('in', 'ten', 'episodes', '@pisodes', 'episodes'),\n",
       " ('ten', 'episodes', 'ok', 'o@', 'of'),\n",
       " ('episodes', 'of', 'thx', 'th@', 'the'),\n",
       " ('of', 'the', 'television', 't@levision', 'television'),\n",
       " ('the', 'television', 'series', 'seri@s', 'series'),\n",
       " ('television', 'series', 'casualty', 'casualt@', 'casualty'),\n",
       " ('series', 'casualty', 'rn', '@n', 'in'),\n",
       " ('casualty', 'in', '<year>', '@year>', '<year>'),\n",
       " ('in', '<year>', 'is', '@s', 'as'),\n",
       " ('<year>', 'as', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('as', '<UNK>', 'fletcher', 'fle@cher', 'fletcher'),\n",
       " ('<UNK>', 'starred', 'rn', '@n', 'in'),\n",
       " ('starred', 'in', 'khe', '@he', 'the'),\n",
       " ('in', 'the', '<year>', '<yea@>', '<year>'),\n",
       " ('the', '<year>', 'film', 'fi@m', 'film'),\n",
       " ('<year>', 'film', 'mercenaries', 'mercenarie@', 'mercenaries'),\n",
       " ('film', 'mercenaries', 'directed', 'd@rected', 'directed'),\n",
       " ('mercenaries', 'directed', 'be', 'b@', 'by'),\n",
       " ('directed', 'by', 'paris', 'pa@is', 'paris'),\n",
       " ('by', 'paris', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('<UNK>', 'starred', 'is', '@s', 'as'),\n",
       " ('starred', 'as', 'scott', 'sc@tt', 'scott'),\n",
       " ('as', 'scott', 'rn', '@n', 'in'),\n",
       " ('scott', 'in', 'toe', 't@e', 'the')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try just spellcheck\n",
    "\n",
    "def spellcheck(masked_words):\n",
    "\n",
    "    preds = []\n",
    "    for word in masked_words:\n",
    "        neighbors = find_candidates(word, vocab, word_to_id_mappings)\n",
    "        \n",
    "        #if there are results, just pick the closest word\n",
    "        if len(neighbors):\n",
    "            pred = neighbors[0]\n",
    "        else: #just predict the unknown token\n",
    "            pred = word_to_id_mappings[\"<UNK>\"]\n",
    "        \n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "preds = spellcheck(masked_words)\n",
    "\n",
    "sp = [(id_to_word_mappings[l[0]], id_to_word_mappings[l[1]], id_to_word_mappings[preds[i]], masked_words[i], id_to_word_mappings[target_tensor.tolist()[i][0]] ) for i, l in enumerate(context_tensor.tolist())]\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.609375"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this looks alot better, lets see how accurate it is\n",
    "\n",
    "np.mean([1 if id_to_word_mappings[preds[i]] == id_to_word_mappings[target_tensor.tolist()[i][0]] else 0 for i, l in enumerate(context_tensor.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 'had', 'x', '@', 'a'),\n",
       " ('had', 'a', 'guest', '@uest', 'guest'),\n",
       " ('a', 'guest', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('guest', '<UNK>', 'starring', 'sta@ring', 'starring'),\n",
       " ('<UNK>', 'starring', 'role', 'r@le', 'role'),\n",
       " ('starring', 'role', 'rn', '@n', 'on'),\n",
       " ('role', 'on', 'thx', 'th@', 'the'),\n",
       " ('on', 'the', 'television', 'televisio@', 'television'),\n",
       " ('the', 'television', 'series', 's@ries', 'series'),\n",
       " ('television', 'series', 'thx', 'th@', 'the'),\n",
       " ('series', 'the', 'bill', 'bil@', 'bill'),\n",
       " ('the', 'bill', 'is', 'i@', 'in'),\n",
       " ('bill', 'in', '<year>', '<yea@>', '<year>'),\n",
       " ('this', 'was', 'followed', 'followe@', 'followed'),\n",
       " ('was', 'followed', 'be', 'b@', 'by'),\n",
       " ('followed', 'by', '', '@', 'a'),\n",
       " ('by', 'a', 'stirring', 'st@rring', 'starring'),\n",
       " ('a', 'starring', 'cole', '@ole', 'role'),\n",
       " ('starring', 'role', 'is', 'i@', 'in'),\n",
       " ('role', 'in', 'thx', 'th@', 'the'),\n",
       " ('in', 'the', 'play', 'p@ay', 'play'),\n",
       " ('the', 'play', 'herons', 'heron@', 'herons'),\n",
       " ('play', 'herons', 'written', 'wr@tten', 'written'),\n",
       " ('herons', 'written', 'by', '@y', 'by'),\n",
       " ('written', 'by', 'simon', 'simo@', 'simon'),\n",
       " ('by', 'simon', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('simon', '<UNK>', 'which', 'whi@h', 'which'),\n",
       " ('<UNK>', 'which', 'was', 'w@s', 'was'),\n",
       " ('which', 'was', 'performed', 'pe@formed', 'performed'),\n",
       " ('was', 'performed', 'is', 'i@', 'in'),\n",
       " ('performed', 'in', '<year>', '<y@ar>', '<year>'),\n",
       " ('in', '<year>', 'aj', 'a@', 'at'),\n",
       " ('<year>', 'at', 'toe', 't@e', 'the'),\n",
       " ('at', 'the', 'royal', 'r@yal', 'royal'),\n",
       " ('the', 'royal', 'court', '@ourt', 'court'),\n",
       " ('royal', 'court', 'theatre', 'theatr@', 'theatre'),\n",
       " ('he', 'had', 'x', '@', 'a'),\n",
       " ('had', 'a', 'guest', 'gu@st', 'guest'),\n",
       " ('a', 'guest', 'role', 'ro@e', 'role'),\n",
       " ('guest', 'role', 'rn', '@n', 'in'),\n",
       " ('role', 'in', 'thx', 'th@', 'the'),\n",
       " ('in', 'the', 'television', 'televis@on', 'television'),\n",
       " ('the', 'television', 'series', '@eries', 'series'),\n",
       " ('television', 'series', 'judge', '@udge', 'judge'),\n",
       " ('series', 'judge', 'john', 'joh@', 'john'),\n",
       " ('judge', 'john', 'dead', 'de@d', 'deed'),\n",
       " ('john', 'deed', 'is', 'i@', 'in'),\n",
       " ('deed', 'in', '<year>', '<y@ar>', '<year>'),\n",
       " ('in', '<year>', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'landed', 'land@d', 'landed'),\n",
       " ('<UNK>', 'landed', '', '@', 'a'),\n",
       " ('landed', 'a', 'role', 'r@le', 'role'),\n",
       " ('a', 'role', 'aj', 'a@', 'as'),\n",
       " ('role', 'as', 'craig', 'cra@g', 'craig'),\n",
       " ('as', 'craig', 'rn', '@n', 'in'),\n",
       " ('craig', 'in', 'thx', 'th@', 'the'),\n",
       " ('in', 'the', 'episode', 'episo@e', 'episode'),\n",
       " ('the', 'episode', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('episode', '<UNK>', \"'a\", \"'@\", \"'s\"),\n",
       " ('<UNK>', \"'s\", 'story', 'stor@', 'story'),\n",
       " (\"'s\", 'story', 'ok', 'o@', 'of'),\n",
       " ('story', 'of', 'khe', '@he', 'the'),\n",
       " ('of', 'the', 'television', 'te@evision', 'television'),\n",
       " ('the', 'television', 'series', 's@ries', 'series'),\n",
       " ('television', 'series', 'thx', 'th@', 'the'),\n",
       " ('series', 'the', 'long', 'lo@g', 'long'),\n",
       " ('the', 'long', 'fire', 'fir@', 'firm'),\n",
       " ('long', 'firm', 'hd', 'h@', 'he'),\n",
       " ('firm', 'he', 'stirred', 'st@rred', 'starred'),\n",
       " ('he', 'starred', 'alongside', 'alon@side', 'alongside'),\n",
       " ('starred', 'alongside', 'actors', 'actor@', 'actors'),\n",
       " ('alongside', 'actors', 'mark', 'm@rk', 'mark'),\n",
       " ('actors', 'mark', 'strong', 'st@ong', 'strong'),\n",
       " ('mark', 'strong', 'abd', 'a@d', 'and'),\n",
       " ('strong', 'and', 'derek', 'dere@', 'derek'),\n",
       " ('and', 'derek', 'jacobi', 'jaco@i', 'jacobi'),\n",
       " ('he', 'was', 'cash', 'cas@', 'cast'),\n",
       " ('was', 'cast', 'rn', '@n', 'in'),\n",
       " ('cast', 'in', 'toe', 't@e', 'the'),\n",
       " ('in', 'the', '<year>', '<ye@r>', '<year>'),\n",
       " ('the', '<year>', 'theatre', 'the@tre', 'theatre'),\n",
       " ('<year>', 'theatre', 'productions', 'pro@uctions', 'productions'),\n",
       " ('theatre', 'productions', 'ok', 'o@', 'of'),\n",
       " ('productions', 'of', 'toe', 't@e', 'the'),\n",
       " ('of', 'the', 'philip', 'phili@', 'philip'),\n",
       " ('the', 'philip', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('philip', '<UNK>', 'play', 'pl@y', 'play'),\n",
       " ('<UNK>', 'play', 'mercury', 'mercu@y', 'mercury'),\n",
       " ('play', 'mercury', 'our', '@ur', 'fur'),\n",
       " ('mercury', 'fur', 'which', 'whic@', 'which'),\n",
       " ('fur', 'which', 'was', 'w@s', 'was'),\n",
       " ('which', 'was', 'performed', 'perfor@ed', 'performed'),\n",
       " ('was', 'performed', 'et', '@t', 'at'),\n",
       " ('performed', 'at', 'thx', 'th@', 'the'),\n",
       " ('at', 'the', 'drum', 'd@um', 'drum'),\n",
       " ('the', 'drum', 'theatre', 'the@tre', 'theatre'),\n",
       " ('drum', 'theatre', 'rn', '@n', 'in'),\n",
       " ('theatre', 'in', 'plymouth', 'ply@outh', 'plymouth'),\n",
       " ('in', 'plymouth', 'ann', 'an@', 'and'),\n",
       " ('plymouth', 'and', 'the', '@he', 'the'),\n",
       " ('and', 'the', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('the', '<UNK>', 'chocolate', 'chocola@e', 'chocolate'),\n",
       " ('<UNK>', 'chocolate', 'factory', 'f@ctory', 'factory'),\n",
       " ('chocolate', 'factory', 'ln', '@n', 'in'),\n",
       " ('factory', 'in', 'lyndon', 'l@ndon', 'london'),\n",
       " ('he', 'was', 'directed', 'direc@ed', 'directed'),\n",
       " ('was', 'directed', 'by', '@y', 'by'),\n",
       " ('directed', 'by', 'john', '@ohn', 'john'),\n",
       " ('by', 'john', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('john', '<UNK>', 'end', '@nd', 'and'),\n",
       " ('<UNK>', 'and', 'started', 'star@ed', 'starred'),\n",
       " ('and', 'starred', 'alongside', 'alongs@de', 'alongside'),\n",
       " ('starred', 'alongside', 'ten', '@en', 'ben'),\n",
       " ('alongside', 'ben', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('ben', '<UNK>', 'shane', 'shan@', 'shane'),\n",
       " ('<UNK>', 'shane', '<UNK>', '<@NK>', '<UNK>'),\n",
       " ('shane', '<UNK>', 'harry', 'h@rry', 'harry'),\n",
       " ('<UNK>', 'harry', 'kent', 'k@nt', 'kent'),\n",
       " ('harry', 'kent', 'fraser', 'frase@', 'fraser'),\n",
       " ('kent', 'fraser', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('fraser', '<UNK>', 'sophie', '@ophie', 'sophie'),\n",
       " ('<UNK>', 'sophie', 'stanton', '@tanton', 'stanton'),\n",
       " ('sophie', 'stanton', 'ann', 'an@', 'and'),\n",
       " ('stanton', 'and', 'dominic', 'd@minic', 'dominic'),\n",
       " ('and', 'dominic', 'gall', '@all', 'hall'),\n",
       " ('in', '<year>', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'starred', 'starr@d', 'starred'),\n",
       " ('<UNK>', 'starred', 'alongside', 'alo@gside', 'alongside'),\n",
       " ('starred', 'alongside', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('alongside', '<UNK>', 'is', 'i@', 'in'),\n",
       " ('<UNK>', 'in', 'khe', '@he', 'the'),\n",
       " ('in', 'the', 'play', '@lay', 'play'),\n",
       " ('the', 'play', 'citizenship', 'citizen@hip', 'citizenship'),\n",
       " ('play', 'citizenship', 'written', 'writte@', 'written'),\n",
       " ('citizenship', 'written', 'by', '@y', 'by'),\n",
       " ('written', 'by', 'mart', 'mar@', 'mark'),\n",
       " ('by', 'mark', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('he', 'appeared', 'rn', '@n', 'on'),\n",
       " ('appeared', 'on', '', '@', 'a'),\n",
       " ('on', 'a', '<year>', '<ye@r>', '<year>'),\n",
       " ('a', '<year>', 'episode', 'epis@de', 'episode'),\n",
       " ('<year>', 'episode', 'sf', '@f', 'of'),\n",
       " ('episode', 'of', 'khe', '@he', 'the'),\n",
       " ('of', 'the', 'television', 'televi@ion', 'television'),\n",
       " ('the', 'television', 'series', '@eries', 'series'),\n",
       " ('television', 'series', 'doctors', 'docto@s', 'doctors'),\n",
       " ('series', 'doctors', 'followed', 'foll@wed', 'followed'),\n",
       " ('doctors', 'followed', 'be', 'b@', 'by'),\n",
       " ('followed', 'by', '', '@', 'a'),\n",
       " ('by', 'a', 'role', 'r@le', 'role'),\n",
       " ('a', 'role', 'rn', '@n', 'in'),\n",
       " ('role', 'in', 'toe', 't@e', 'the'),\n",
       " ('in', 'the', '<year>', '<y@ar>', '<year>'),\n",
       " ('the', '<year>', 'theatre', 'theatr@', 'theatre'),\n",
       " ('<year>', 'theatre', 'production', 'producti@n', 'production'),\n",
       " ('theatre', 'production', 'sf', '@f', 'of'),\n",
       " ('production', 'of', 'how', 'h@w', 'how'),\n",
       " ('of', 'how', 'po', '@o', 'to'),\n",
       " ('how', 'to', 'curse', 'curs@', 'curse'),\n",
       " ('to', 'curse', 'directed', 'direct@d', 'directed'),\n",
       " ('curse', 'directed', 'be', 'b@', 'by'),\n",
       " ('directed', 'by', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('by', '<UNK>', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('how', 'to', 'curse', 'c@rse', 'curse'),\n",
       " ('to', 'curse', 'bas', '@as', 'was'),\n",
       " ('curse', 'was', 'performed', '@erformed', 'performed'),\n",
       " ('was', 'performed', 'et', '@t', 'at'),\n",
       " ('performed', 'at', 'bush', 'bus@', 'bush'),\n",
       " ('at', 'bush', 'theatre', 't@eatre', 'theatre'),\n",
       " ('bush', 'theatre', 'is', 'i@', 'in'),\n",
       " ('theatre', 'in', 'toe', 't@e', 'the'),\n",
       " ('in', 'the', 'london', 'lon@on', 'london'),\n",
       " ('the', 'london', 'borough', 'bo@ough', 'borough'),\n",
       " ('london', 'borough', 'sf', '@f', 'of'),\n",
       " ('borough', 'of', '<UNK>', '<@NK>', '<UNK>'),\n",
       " ('of', '<UNK>', 'end', '@nd', 'and'),\n",
       " ('<UNK>', 'and', 'fulham', '@ulham', 'fulham'),\n",
       " ('<UNK>', 'starred', 'is', 'i@', 'in'),\n",
       " ('starred', 'in', 'too', 't@o', 'two'),\n",
       " ('in', 'two', 'films', 'film@', 'films'),\n",
       " ('two', 'films', 'is', 'i@', 'in'),\n",
       " ('films', 'in', '<year>', '<@ear>', '<year>'),\n",
       " ('in', '<year>', 'daylight', 'dayligh@', 'daylight'),\n",
       " ('<year>', 'daylight', 'robbery', 'rob@ery', 'robbery'),\n",
       " ('daylight', 'robbery', 'by', '@y', 'by'),\n",
       " ('robbery', 'by', 'filmmaker', 'f@lmmaker', 'filmmaker'),\n",
       " ('by', 'filmmaker', 'paris', 'pa@is', 'paris'),\n",
       " ('filmmaker', 'paris', '<UNK>', '<UN@>', '<UNK>'),\n",
       " ('paris', '<UNK>', 'abd', 'a@d', 'and'),\n",
       " ('<UNK>', 'and', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('and', '<UNK>', 'lunch', '@unch', 'punch'),\n",
       " ('<UNK>', 'punch', 'directed', 'd@rected', 'directed'),\n",
       " ('punch', 'directed', 'by', '@y', 'by'),\n",
       " ('directed', 'by', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('by', '<UNK>', 'blackburn', 'b@ackburn', 'blackburn'),\n",
       " ('in', 'may', '<year>', '@year>', '<year>'),\n",
       " ('may', '<year>', '<UNK>', '@UNK>', '<UNK>'),\n",
       " ('<year>', '<UNK>', 'made', 'mad@', 'made'),\n",
       " ('<UNK>', 'made', '', '@', 'a'),\n",
       " ('made', 'a', 'guest', 'gue@t', 'guest'),\n",
       " ('a', 'guest', 'appearance', 'a@pearance', 'appearance'),\n",
       " ('guest', 'appearance', 'ok', 'o@', 'on'),\n",
       " ('appearance', 'on', '', '@', 'a'),\n",
       " ('on', 'a', 'too', 't@o', 'two'),\n",
       " ('a', 'two', '<UNK>', '<@NK>', '<UNK>'),\n",
       " ('two', '<UNK>', 'part', 'par@', 'part'),\n",
       " ('<UNK>', 'part', 'episode', 'e@isode', 'episode'),\n",
       " ('part', 'episode', 'ari', 'ar@', 'arc'),\n",
       " ('episode', 'arc', 'sf', '@f', 'of'),\n",
       " ('arc', 'of', 'khe', '@he', 'the'),\n",
       " ('of', 'the', 'television', 'tele@ision', 'television'),\n",
       " ('the', 'television', 'series', '@eries', 'series'),\n",
       " ('television', 'series', 'waking', 'wakin@', 'waking'),\n",
       " ('series', 'waking', 'toe', 't@e', 'the'),\n",
       " ('waking', 'the', 'dead', 'de@d', 'dead'),\n",
       " ('the', 'dead', 'followed', 'foll@wed', 'followed'),\n",
       " ('dead', 'followed', 'be', 'b@', 'by'),\n",
       " ('followed', 'by', 'rn', '@n', 'an'),\n",
       " ('by', 'an', 'appearance', 'appea@ance', 'appearance'),\n",
       " ('an', 'appearance', 'ok', 'o@', 'on'),\n",
       " ('appearance', 'on', 'toe', 't@e', 'the'),\n",
       " ('on', 'the', 'television', 'televisio@', 'television'),\n",
       " ('the', 'television', 'series', 'se@ies', 'series'),\n",
       " ('television', 'series', 'survivors', 'sur@ivors', 'survivors'),\n",
       " ('series', 'survivors', 'rn', '@n', 'in'),\n",
       " ('survivors', 'in', 'november', 'novemb@r', 'november'),\n",
       " ('in', 'november', '<year>', '<ye@r>', '<year>'),\n",
       " ('he', 'had', 'x', '@', 'a'),\n",
       " ('had', 'a', 'recurring', 'recurri@g', 'recurring'),\n",
       " ('a', 'recurring', 'cole', '@ole', 'role'),\n",
       " ('recurring', 'role', 'is', 'i@', 'in'),\n",
       " ('role', 'in', 'ten', 'te@', 'ten'),\n",
       " ('in', 'ten', 'episodes', '@pisodes', 'episodes'),\n",
       " ('ten', 'episodes', 'ok', 'o@', 'of'),\n",
       " ('episodes', 'of', 'thx', 'th@', 'the'),\n",
       " ('of', 'the', 'television', 't@levision', 'television'),\n",
       " ('the', 'television', 'series', 'seri@s', 'series'),\n",
       " ('television', 'series', 'casualty', 'casualt@', 'casualty'),\n",
       " ('series', 'casualty', 'rn', '@n', 'in'),\n",
       " ('casualty', 'in', '<year>', '@year>', '<year>'),\n",
       " ('in', '<year>', 'is', '@s', 'as'),\n",
       " ('<year>', 'as', '<UNK>', '<UNK@', '<UNK>'),\n",
       " ('as', '<UNK>', 'fletcher', 'fle@cher', 'fletcher'),\n",
       " ('<UNK>', 'starred', 'rn', '@n', 'in'),\n",
       " ('starred', 'in', 'khe', '@he', 'the'),\n",
       " ('in', 'the', '<year>', '<yea@>', '<year>'),\n",
       " ('the', '<year>', 'film', 'fi@m', 'film'),\n",
       " ('<year>', 'film', 'mercenaries', 'mercenarie@', 'mercenaries'),\n",
       " ('film', 'mercenaries', 'directed', 'd@rected', 'directed'),\n",
       " ('mercenaries', 'directed', 'be', 'b@', 'by'),\n",
       " ('directed', 'by', 'paris', 'pa@is', 'paris'),\n",
       " ('by', 'paris', '<UNK>', '<U@K>', '<UNK>'),\n",
       " ('<UNK>', 'starred', 'is', '@s', 'as'),\n",
       " ('starred', 'as', 'scott', 'sc@tt', 'scott'),\n",
       " ('as', 'scott', 'rn', '@n', 'in'),\n",
       " ('scott', 'in', 'toe', 't@e', 'the')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets try combining these approaches\n",
    "\n",
    "def spellcheck_with_NN(log_probs, masked_words):\n",
    "\n",
    "    #NN probs\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # Sort indices based on probabilities, each indice corresponds to a word id\n",
    "    sorted_indices = torch.argsort(probs, descending=True, axis=1).tolist()\n",
    "\n",
    "    preds = []\n",
    "    for i, word in enumerate(masked_words):\n",
    "        neighbors = find_candidates(word, vocab, word_to_id_mappings)\n",
    "\n",
    "        NN_preds = sorted_indices[i]\n",
    "\n",
    "        #iterate through words first based on distance, and see how they match with the NN probs, the first match will be our pred\n",
    "\n",
    "        highest_prob_index = len(NNSPpreds)\n",
    "        pred = neighbors[0] #default prediction if no match\n",
    "        for neighbor in neighbors:\n",
    "            #only search up till previous highest probabilty match\n",
    "\n",
    "            j = 0\n",
    "            while j < highest_prob_index:\n",
    "                id = NN_preds[j]\n",
    "                if neighbor == id:\n",
    "                    pred = id\n",
    "                    highest_prob_index = j\n",
    "                j += 1\n",
    "  \n",
    "  \n",
    "        \n",
    "        preds.append(pred)\n",
    "           \n",
    "\n",
    "    \n",
    "    return preds\n",
    "\n",
    "NNSPpreds = spellcheck_with_NN(log_probs, masked_words)\n",
    "\n",
    "nnsp = [(id_to_word_mappings[l[0]], id_to_word_mappings[l[1]], id_to_word_mappings[NNSPpreds[i]], masked_words[i], id_to_word_mappings[target_tensor.tolist()[i][0]] ) for i, l in enumerate(context_tensor.tolist())]\n",
    "nnsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61328125"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1 if id_to_word_mappings[NNSPpreds[i]] == id_to_word_mappings[target_tensor.tolist()[i][0]] else 0 for i, l in enumerate(context_tensor.tolist())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding bengio only slightly improves it\n",
    "\n",
    "note: try using something like word2vec as a supplement instead of bengio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
