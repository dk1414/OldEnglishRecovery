{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidates(word, vocabulary, word_to_id_map, missing_token=\"@\"):\n",
    "    candidates = []\n",
    "\n",
    "    for vocab_word in vocabulary:\n",
    "        if len(word) != len(vocab_word):\n",
    "            continue  # Skip words with different lengths\n",
    "\n",
    "        candidate = []\n",
    "        for char1, char2 in zip(word, vocab_word):\n",
    "            if char1 == missing_token:\n",
    "                candidate.append(char2)\n",
    "            elif char1 == char2:\n",
    "                candidate.append(char2)  \n",
    "            else:\n",
    "                break #mismatch, skip\n",
    "        else:\n",
    "            try:\n",
    "                candidates.append(word_to_id_map[\"\".join(candidate)])\n",
    "            except:\n",
    "                print(word, vocab_word, candidate,\"\".join(candidate) )\n",
    "                raise Exception\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77363\n",
      "9418\n"
     ]
    }
   ],
   "source": [
    "#load wikitext data\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "train_file_path = '../Srilm/newtraincorpus.txt'\n",
    "test_file_path = '../Srilm/newtestcorpus.txt'\n",
    "\n",
    "with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "            # Process each line as a sentence\n",
    "            words = (line.strip().split())\n",
    "            train.append(words)\n",
    "\n",
    "with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "            # Process each line as a sentence\n",
    "            words = (line.strip().split())\n",
    "            test.append(words)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19114\n"
     ]
    }
   ],
   "source": [
    "#create vocabulary\n",
    "\n",
    "UNK_symbol = \"<UNK>\"\n",
    "vocab = set([UNK_symbol])\n",
    "\n",
    "\n",
    "\n",
    "# create term frequency of the words\n",
    "words_term_frequency_train = {}\n",
    "for doc in train:\n",
    "    for word in doc:\n",
    "        # this will calculate term frequency\n",
    "        # since we are taking all words now\n",
    "        words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "\n",
    "# create vocabulary\n",
    "for doc in train:\n",
    "    for word in doc:\n",
    "        if words_term_frequency_train.get(word,0) >= 5:\n",
    "            vocab.add(word)\n",
    "\n",
    "# remove \"@-@\" from vocab\n",
    "vocab.remove(\"@-@\")\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164980, 3)\n",
      "(164980, 1)\n"
     ]
    }
   ],
   "source": [
    "#create 4grams\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "# create word to id mappings\n",
    "word_to_id_mappings = {}\n",
    "for idx,word in enumerate(vocab):\n",
    "    word_to_id_mappings[word] = idx\n",
    "\n",
    "id_to_word_mappings = {v: k for k, v in word_to_id_mappings.items()}\n",
    "\n",
    "# function to get id for a given word\n",
    "# return <UNK> id if not found\n",
    "def get_id_of_word(word):\n",
    "    unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "    return word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "\n",
    "for sentence in test:\n",
    "    for i,word in enumerate(sentence):\n",
    "        if i+3 >= len(sentence):\n",
    "            # sentence boundary reached\n",
    "            # ignoring sentence less than 3 words\n",
    "            break\n",
    "        # convert word to id\n",
    "        x_extract = [get_id_of_word(word),get_id_of_word(sentence[i+1]), get_id_of_word(sentence[i+2])]\n",
    "        y_extract = [get_id_of_word(sentence[i+3])]\n",
    "\n",
    "        x_test.append(x_extract)\n",
    "        y_test.append(y_extract)\n",
    "  \n",
    "\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)  \n",
    "  \n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "#shuffle data cause this is too much for this \n",
    "x_test, y_test = shuffle(x_test, y_test)\n",
    "\n",
    "x_test = x_test[0:10000]\n",
    "y_test = y_test[0:10000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataset will add a version of the label, (third word in the trigram), with a single character removed \n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "import random\n",
    "\n",
    "def randomly_replace_char(input_str, missing_token=\"@\"):\n",
    "    if not input_str:\n",
    "        return input_str  \n",
    "\n",
    "    num_replacements = random.randint(1, min(len(input_str), 3))  \n",
    "    indices_to_replace = random.sample(range(len(input_str)), num_replacements)\n",
    "\n",
    "    modified_str = input_str\n",
    "    for index in indices_to_replace:\n",
    "        modified_str = modified_str[:index] + missing_token + modified_str[index + 1:]\n",
    "\n",
    "    return modified_str\n",
    "\n",
    "class charMaskDataset(Dataset):\n",
    "    def __init__(self, data, labels, id_to_word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of input data samples.\n",
    "            labels (list): List of corresponding labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.id_to_word = id_to_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for index in range(len(self)):\n",
    "            yield self[index]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if index < len(self.data):\n",
    "\n",
    "            context = [self.id_to_word[word] for word in self.data[index]]\n",
    "            label = self.id_to_word[self.labels[index][0]]\n",
    "\n",
    "\n",
    "            #randomly remove a single char from the label word\n",
    "\n",
    "            masked_word = randomly_replace_char(label)\n",
    "\n",
    "    \n",
    "\n",
    "            return context, label, masked_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = charMaskDataset(x_test, y_test, id_to_word_mappings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_similarity(context, target):\n",
    "\n",
    "    avg = 0\n",
    "    target = nlp(target)\n",
    "    for word in context:\n",
    "        avg += target.similarity(nlp(word))\n",
    "\n",
    "    return avg / len(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):\n",
    "\n",
    "    preds = []\n",
    "    true = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "\n",
    "        \n",
    "        context, label, masked_word = example\n",
    "\n",
    "        pred = \"<UNK>\"\n",
    "        pred_score = -1\n",
    "        candidates = find_candidates(masked_word, vocab, word_to_id_mappings)\n",
    "\n",
    "        for cand in candidates:\n",
    "            cand = id_to_word_mappings[cand]\n",
    "\n",
    "            score = compute_avg_similarity(context, cand)\n",
    "\n",
    "            if score > pred_score:\n",
    "                pred = cand\n",
    "                pred_score = score\n",
    "        \n",
    "        preds.append(pred)\n",
    "        true.append(label)\n",
    "\n",
    "    \n",
    "    return preds, true\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\declan\\AppData\\Local\\Temp\\ipykernel_32796\\2316823434.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  avg += target.similarity(nlp(word))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Acc: 0.6806\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     masked_totals[masks[i]\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc by num chars missing: 0 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmasked_correct\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmasked_totals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 1 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmasked_correct[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mmasked_totals[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 2 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmasked_correct[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mmasked_totals[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 3 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmasked_correct[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mmasked_totals[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "predictions, true_labels = predict(dataset)\n",
    "\n",
    "correct = 0\n",
    "masked_correct = [0,0,0,0]\n",
    "masked_totals = [0,0,0,0]\n",
    "\n",
    "masks = [e[2] for e in dataset]\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    if p == true_labels[i]:\n",
    "        correct += 1\n",
    "\n",
    "        #see how it does depending on how masked the word is\n",
    "        masked_correct[masks[i].count('@')] += 1\n",
    "\n",
    "    masked_totals[masks[i].count('@')] += 1\n",
    "\n",
    "print(f'Total Acc: {correct / len(predictions)}')\n",
    "print(f'Acc by num chars missing: 0 : {masked_correct[0] / masked_totals[0]}, 1 : {masked_correct[1] / masked_totals[1]}, 2 : {masked_correct[2] / masked_totals[2]}, 3 : {masked_correct[3] / masked_totals[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['in', 'other', 'aspects'], 'especially', 'e@peci@l@y')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2499, 2299, 2008]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3805, 3480, 2715]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Acc: 0.6806\n",
      "Acc by num chars missing:  1 : 0.6567674113009199, 2 : 0.660632183908046, 3 : 0.7395948434622468\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Acc: {correct / len(predictions)}')\n",
    "print(f'Acc by num chars missing:  1 : {masked_correct[1] / masked_totals[1]}, 2 : {masked_correct[2] / masked_totals[2]}, 3 : {masked_correct[3] / masked_totals[3]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
